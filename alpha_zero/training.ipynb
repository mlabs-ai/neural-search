{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains the AlphaZero agent on a single machine for the game of Go.\"\"\"\n",
    "import os\n",
    "\n",
    "# This forces OpenMP to use 1 single thread, which is needed to\n",
    "# prevent contention between multiple process.\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# Tell numpy to only use one core.\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('board_size', 9, 'Board size for Go.')\n",
    "flags.DEFINE_float('komi', 7.5, 'Komi rule for Go.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_stack',\n",
    "    8,\n",
    "    'Stack N previous states, the state is an image of N x 2 + 1 binary planes.',\n",
    ")\n",
    "\n",
    "\n",
    "flags.DEFINE_integer('num_res_blocks', 10, 'Number of residual blocks in the neural network.')\n",
    "\n",
    "flags.DEFINE_integer('num_filters', 128, 'Number of filters for the conv2d layers in the neural network.')\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'num_fc_units',\n",
    "    128,\n",
    "    'Number of hidden units in the linear layer of the neural network.',\n",
    ")\n",
    "\n",
    "flags.DEFINE_integer('min_games', 20000, 'Collect number of self-play games before learning starts.')\n",
    "flags.DEFINE_integer(\n",
    "    'games_per_ckpt',\n",
    "    100,\n",
    "    'Collect minimum number of self-play games using the last checkpoint before creating the next checkpoint.',\n",
    ")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'replay_capacity',\n",
    "    250000 * 50,\n",
    "    'Replay buffer capacity is number of game * average game length.' 'Note, 250000 games may need ~30GB of RAM',\n",
    ")\n",
    "flags.DEFINE_integer(\n",
    "    'batch_size',\n",
    "    1024,\n",
    "    'To avoid overfitting, we want to make sure the agent only sees ~10% of samples in the replay over one checkpoint.'\n",
    "    'That is, batch_size * ckpt_interval <= replay_capacity * 0.1',\n",
    ")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    'argument_data',\n",
    "    True,\n",
    "    'Apply random rotation and mirroring to the training data, default on.',\n",
    ")\n",
    "flags.DEFINE_bool('compress_data', False, 'Compress state when saving in replay buffer, default off.')\n",
    "\n",
    "flags.DEFINE_float('init_lr', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_float('lr_decay', 0.1, 'Learning rate decay rate.')\n",
    "flags.DEFINE_multi_integer(\n",
    "    'lr_milestones',\n",
    "    [10000, 20000, 40000],\n",
    "    'The number of training steps at which the learning rate will be decayed.',\n",
    ")\n",
    "flags.DEFINE_float('l2_regularization', 1e-4, 'The L2 regularization parameter applied to weights.')\n",
    "flags.DEFINE_float('sgd_momentum', 0.9, '')\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'max_training_steps',\n",
    "    int(5e4),\n",
    "    'Number of training steps (measured in network parameter update, one batch is one training step).',\n",
    ")\n",
    "flags.DEFINE_integer('num_actors',32, 'Number of self-play actor processes.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_simulations',\n",
    "    200,\n",
    "    'Number of simulations per MCTS search, this applies to both self-play and evaluation processes.',\n",
    ")\n",
    "flags.DEFINE_integer(\n",
    "    'num_parallel',\n",
    "    8,\n",
    "    'Number of leaves to collect before using the neural network to evaluate the positions during MCTS search,'\n",
    "    '1 means no parallel search.',\n",
    ")\n",
    "flags.DEFINE_float(\n",
    "    'c_puct_base',\n",
    "    19652,\n",
    "    'Exploration constants balancing priors vs. search values. Original paper use 19652',\n",
    ")\n",
    "flags.DEFINE_float(\n",
    "    'c_puct_init',\n",
    "    1.25,\n",
    "    'Exploration constants balancing priors vs. search values. Original paper use 1.25',\n",
    ")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'warm_up_steps',\n",
    "    16,\n",
    "    'Number of steps at the beginning of a self-play game where the search temperature is set to 1.',\n",
    ")\n",
    "flags.DEFINE_float(\n",
    "    'init_resign_threshold',\n",
    "    -0.88,\n",
    "    'The self-play game is resigned if MCTS search values are lesser than this threshold.'\n",
    "    'This value is also dynamically adjusted (decreased) during training to keep the false positive below the target level.'\n",
    "    '-1 means no resign and it disables all the features related to resignations during self-play.',\n",
    ")\n",
    "flags.DEFINE_integer(\n",
    "    'check_resign_after_steps',\n",
    "    40,\n",
    "    'Number steps into the self-play game before checking for resign.',\n",
    ")\n",
    "flags.DEFINE_float(\n",
    "    'target_fp_rate',\n",
    "    0.05,\n",
    "    'Target resignation false positives rate, the resignation threshold is dynamically adjusted to keep the false positives rate below this value.',\n",
    ")\n",
    "flags.DEFINE_float(\n",
    "    'disable_resign_ratio',\n",
    "    0.1,\n",
    "    'Disable resign for proportion of self-play games so we can measure resignation false positives.',\n",
    ")\n",
    "flags.DEFINE_integer(\n",
    "    'reset_fp_interval',\n",
    "    100000,\n",
    "    'The frequency (measured in number of self-play games) to reset resignation threshold,'\n",
    "    'so statistics from old games do not influence current play.',\n",
    ")\n",
    "flags.DEFINE_integer(\n",
    "    'no_resign_games',\n",
    "    50000,\n",
    "    'Initial games played with resignation disable. '\n",
    "    'This makes sense as when starting out, the prediction from the neural network is not accurate.',\n",
    ")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    'default_rating',\n",
    "    0,\n",
    "    'Default elo rating, change to the rating (for black) from last checkpoint when resume training.',\n",
    ")\n",
    "flags.DEFINE_integer('ckpt_interval', 500, 'The frequency (in training step) to create new checkpoint.')\n",
    "flags.DEFINE_integer('log_interval', 200, 'The frequency (in training step) to log training statistics.')\n",
    "flags.DEFINE_string('ckpt_dir', './checkpoints/go/9x9/resnets', 'Path for checkpoint file.')\n",
    "flags.DEFINE_string(\n",
    "    'logs_dir',\n",
    "    './logs/go/9x9/resnets',\n",
    "    'Path to save statistics for self-play, training, and evaluation.',\n",
    ")\n",
    "flags.DEFINE_string(\n",
    "    'dataset_dir',\n",
    "    'go_dataset.pth',\n",
    "    'Go dataset',\n",
    ")\n",
    "flags.DEFINE_string(\n",
    "    'eval_games_dir',\n",
    "    './games/pro_games/go/9x9',\n",
    "    'Path contains evaluation games in sgf format.',\n",
    ")\n",
    "flags.DEFINE_string(\n",
    "    'save_sgf_dir',\n",
    "    './games/selfplay_games/go/9x9',\n",
    "    'Path to selfplay and evaluation games in sgf format.',\n",
    ")\n",
    "flags.DEFINE_integer('save_sgf_interval', 500, 'How often to save self-play games.')\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'save_replay_interval',\n",
    "    0,\n",
    "    'The frequency (in number of self-play games) to save the replay buffer state.'\n",
    "    'So we can resume training without staring from zero. 0 means do not save replay state.'\n",
    "    'If you set this to a non-zero value, you should make sure the path specified by \"FLAGS.ckpt_dir\" have at least 100GB of free space.',\n",
    ")\n",
    "flags.DEFINE_string('load_ckpt', '', 'Resume training by starting from last checkpoint.')\n",
    "flags.DEFINE_string('load_replay', '', 'Resume training by loading saved replay buffer state.')\n",
    "\n",
    "flags.DEFINE_string('log_level', 'INFO', '')\n",
    "flags.DEFINE_integer('seed', 1, 'Seed the runtime.')\n",
    "\n",
    "flags.register_validator('num_simulations', lambda x: x > 1)\n",
    "flags.register_validator('log_level', lambda x: x in ['INFO', 'DEBUG'])\n",
    "flags.register_multi_flags_validator(\n",
    "    ['num_parallel', 'c_puct_base'],\n",
    "    lambda flags: flags['c_puct_base'] >= 19652 * (flags['num_parallel'] / 800),\n",
    "    '',\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize flags\n",
    "FLAGS(sys.argv, known_only = True)\n",
    "\n",
    "os.environ['BOARD_SIZE'] = str(FLAGS.board_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_zero.envs.go import GoEnv\n",
    "from alpha_zero.core.pipeline import (\n",
    "    supervised_learner_loop,\n",
    "    set_seed,\n",
    "    maybe_create_dir,\n",
    ")\n",
    "from alpha_zero.core.network import AlphaZeroNet\n",
    "from alpha_zero.utils.util import extract_args_from_flags_dict, create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_builder():\n",
    "        return GoEnv(komi=FLAGS.komi, num_stack=FLAGS.num_stack)\n",
    "eval_env = env_builder()\n",
    "\n",
    "input_shape = eval_env.observation_space.shape\n",
    "num_actions = eval_env.action_space.n\n",
    "def network_builder():\n",
    "        return AlphaZeroNet(\n",
    "            input_shape,\n",
    "            num_actions,\n",
    "            FLAGS.num_res_blocks,\n",
    "            FLAGS.num_filters,\n",
    "            FLAGS.num_fc_units,\n",
    "        )\n",
    "network = network_builder()\n",
    "optimizer = torch.optim.SGD(\n",
    "    network.parameters(),\n",
    "    lr=FLAGS.init_lr,\n",
    "    momentum=FLAGS.sgd_momentum,\n",
    "    weight_decay=FLAGS.l2_regularization,\n",
    ")\n",
    "lr_scheduler = MultiStepLR(optimizer, milestones=FLAGS.lr_milestones, gamma=FLAGS.lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in network.parameters())\n",
    "print(f\" Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(FLAGS.seed)\n",
    "\n",
    "maybe_create_dir(FLAGS.ckpt_dir)\n",
    "maybe_create_dir(FLAGS.logs_dir)\n",
    "maybe_create_dir(FLAGS.save_sgf_dir)\n",
    "\n",
    "logger = create_logger(FLAGS.log_level)\n",
    "\n",
    "logger.info(extract_args_from_flags_dict(FLAGS.flag_values_dict()))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    learner_device = torch.device('cuda')\n",
    "supervised_learner_loop(\n",
    "    seed = FLAGS.seed,\n",
    "    network = network,\n",
    "    data_dir = FLAGS.dataset_dir,\n",
    "    device = learner_device,\n",
    "    optimizer = optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    logger = logger,\n",
    "    argument_data = FLAGS.argument_data,\n",
    "    batch_size = FLAGS.batch_size,\n",
    "    ckpt_interval = FLAGS.ckpt_interval,\n",
    "    log_interval = FLAGS.log_interval,\n",
    "    max_training_steps = FLAGS.max_training_steps,\n",
    "    patience = 10,\n",
    "    ckpt_dir = FLAGS.ckpt_dir,\n",
    "    logs_dir = FLAGS.logs_dir,\n",
    "   )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
