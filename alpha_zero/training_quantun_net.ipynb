{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains the AlphaZero agent on a single machine for the game of Go.\"\"\"\n",
    "import os\n",
    "\n",
    "# This forces OpenMP to use 1 single thread, which is needed to\n",
    "# prevent contention between multiple process.\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# Tell numpy to only use one core.\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "import sys\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('board_size', 9, 'Board size for Go.')\n",
    "flags.DEFINE_float('komi', 7.5, 'Komi rule for Go.')\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'num_stack',\n",
    "    8,\n",
    "    'Stack N previous states, the state is an image of N x 2 + 1 binary planes.',\n",
    ")\n",
    "\n",
    "flags.DEFINE_integer('num_filters', 128, 'Number of filters for the conv2d layers in the neural network.')\n",
    "flags.DEFINE_integer('max_depth', 5, ' maximum depth for quantum search')\n",
    "flags.DEFINE_integer('branching_width', 3, ' branching_width for quantum search')\n",
    "flags.DEFINE_integer('beam_width', 1, ' beam_width for quantum search')\n",
    "flags.DEFINE_integer(\n",
    "    'num_fc_units',\n",
    "    128,\n",
    "    'Number of hidden units in the linear layer of the neural network.',\n",
    ")\n",
    "flags.DEFINE_integer('num_search', 2, ' number of search modules for quantum search')\n",
    "\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'batch_size',\n",
    "    1024,\n",
    "    'To avoid overfitting, we want to make sure the agent only sees ~10% of samples in the replay over one checkpoint.'\n",
    "    'That is, batch_size * ckpt_interval <= replay_capacity * 0.1',\n",
    ")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    'argument_data',\n",
    "    True,\n",
    "    'Apply random rotation and mirroring to the training data, default on.',\n",
    ")\n",
    "\n",
    "\n",
    "flags.DEFINE_float('init_lr', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_float('lr_decay', 0.1, 'Learning rate decay rate.')\n",
    "flags.DEFINE_multi_integer(\n",
    "    'lr_milestones',\n",
    "    [10000, 20000, 40000],\n",
    "    'The number of training steps at which the learning rate will be decayed.',\n",
    ")\n",
    "flags.DEFINE_float('l2_regularization', 1e-4, 'The L2 regularization parameter applied to weights.')\n",
    "flags.DEFINE_float('sgd_momentum', 0.9, '')\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'max_training_steps',\n",
    "    int(5e4),\n",
    "    'Number of training steps (measured in network parameter update, one batch is one training step).',\n",
    ")\n",
    "\n",
    "flags.DEFINE_integer('ckpt_interval', 500, 'The frequency (in training step) to create new checkpoint.')\n",
    "flags.DEFINE_integer('log_interval', 20, 'The frequency (in training step) to log training statistics.')\n",
    "\n",
    "flags.DEFINE_string('ckpt_dir', '', 'Checkpoint directory (to be generated dynamically)')\n",
    "flags.DEFINE_string('logs_dir', '', 'Logs directory (to be generated dynamically)')\n",
    "flags.DEFINE_string(\n",
    "    'dataset_dir',\n",
    "    'go_dataset.pth',\n",
    "    'Go dataset',\n",
    ")\n",
    "\n",
    "flags.DEFINE_string('log_level', 'INFO', '')\n",
    "flags.DEFINE_integer('seed', 1, 'Seed the runtime.')\n",
    "\n",
    "\n",
    "# Initialize flags\n",
    "FLAGS(sys.argv, known_only = True)\n",
    "\n",
    "os.environ['BOARD_SIZE'] = str(FLAGS.board_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_folder_name(depth, search, branching, filters, beam):\n",
    "    return f\"d_{depth}s_{search}br_{branching}f_{filters}be_{beam}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = generate_folder_name(\n",
    "        FLAGS.max_depth, FLAGS.num_search, FLAGS.branching_width, FLAGS.num_filters, FLAGS.beam_width\n",
    "    )\n",
    "\n",
    "# Update ckpt_dir and logs_dir with the generated folder name\n",
    "FLAGS.ckpt_dir = f'./checkpoints/go/9x9/quantum/{folder_name}'\n",
    "FLAGS.logs_dir = f'./logs/go/9x9/quantum/{folder_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plink failed to import tkinter.\n"
     ]
    }
   ],
   "source": [
    "from alpha_zero.envs.go import GoEnv\n",
    "from alpha_zero.core.pipeline import (\n",
    "    supervised_learner_loop,\n",
    "    set_seed,\n",
    "    maybe_create_dir,\n",
    ")\n",
    "from alpha_zero.core.quantum_net import QuantumAlphaZeroNet\n",
    "from alpha_zero.utils.util import extract_args_from_flags_dict, create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_builder():\n",
    "        return GoEnv(komi=FLAGS.komi, num_stack=FLAGS.num_stack)\n",
    "eval_env = env_builder()\n",
    "\n",
    "input_shape = eval_env.observation_space.shape\n",
    "num_actions = eval_env.action_space.n\n",
    "def network_builder():\n",
    "        return QuantumAlphaZeroNet(\n",
    "            input_shape,\n",
    "            num_actions,\n",
    "            FLAGS.num_filters,\n",
    "            FLAGS.max_depth,\n",
    "            FLAGS.branching_width,\n",
    "            FLAGS.beam_width,\n",
    "            FLAGS.num_fc_units,\n",
    "            FLAGS.num_search\n",
    "\n",
    "        )\n",
    "network = network_builder()\n",
    "network = torch.compile(network)\n",
    "optimizer = torch.optim.SGD(\n",
    "    network.parameters(),\n",
    "    lr=FLAGS.init_lr,\n",
    "    momentum=FLAGS.sgd_momentum,\n",
    "    weight_decay=FLAGS.l2_regularization,\n",
    ")\n",
    "lr_scheduler = MultiStepLR(optimizer, milestones=FLAGS.lr_milestones, gamma=FLAGS.lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of parameters: 1728769\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in network.parameters())\n",
    "print(f\" Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 9, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2024-11-30 11:46:02 501677995.py:11] {'board_size': 9, 'komi': 7.5, 'num_stack': 8, 'num_filters': 128, 'max_depth': 5, 'branching_width': 3, 'beam_width': 1, 'num_fc_units': 128, 'num_search': 2, 'batch_size': 1024, 'argument_data': True, 'init_lr': 0.01, 'lr_decay': 0.1, 'lr_milestones': [10000, 20000, 40000], 'l2_regularization': 0.0001, 'sgd_momentum': 0.9, 'max_training_steps': 50000, 'ckpt_interval': 500, 'log_interval': 20, 'ckpt_dir': './checkpoints/go/9x9/quantum/d_5s_2br_3f_128be_1', 'logs_dir': './logs/go/9x9/quantum/d_5s_2br_3f_128be_1', 'dataset_dir': 'go_dataset.pth', 'log_level': 'INFO', 'seed': 1}\n",
      "/home/banashree/neural-search/alpha_zero/core/pipeline.py:669: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  go_dataset = torch.load(data_dir)\n",
      "INFO 2024-11-30 11:47:09 pipeline.py:712] Training Step 20: Policy loss = 4.397378921508789, value loss = 0.9417761564254761\n",
      "INFO 2024-11-30 11:47:55 pipeline.py:712] Training Step 40: Policy loss = 4.358293533325195, value loss = 0.934546709060669\n",
      "INFO 2024-11-30 11:48:47 pipeline.py:712] Training Step 60: Policy loss = 4.323507308959961, value loss = 0.9457874298095703\n",
      "INFO 2024-11-30 11:49:43 pipeline.py:712] Training Step 80: Policy loss = 4.2925872802734375, value loss = 0.9511508941650391\n",
      "INFO 2024-11-30 11:50:36 pipeline.py:712] Training Step 100: Policy loss = 4.237202167510986, value loss = 0.9396582841873169\n",
      "INFO 2024-11-30 11:51:30 pipeline.py:712] Training Step 120: Policy loss = 4.228631019592285, value loss = 0.9279808402061462\n",
      "INFO 2024-11-30 11:52:23 pipeline.py:712] Training Step 140: Policy loss = 4.204631805419922, value loss = 0.9427284598350525\n",
      "INFO 2024-11-30 11:53:17 pipeline.py:712] Training Step 160: Policy loss = 4.17225456237793, value loss = 0.9327467679977417\n",
      "INFO 2024-11-30 11:54:10 pipeline.py:712] Training Step 180: Policy loss = 4.190344333648682, value loss = 0.9453011751174927\n",
      "INFO 2024-11-30 11:55:04 pipeline.py:712] Training Step 200: Policy loss = 4.160862922668457, value loss = 0.9416103363037109\n",
      "INFO 2024-11-30 11:55:58 pipeline.py:712] Training Step 220: Policy loss = 4.155803203582764, value loss = 0.9346502423286438\n",
      "INFO 2024-11-30 11:56:51 pipeline.py:712] Training Step 240: Policy loss = 4.146127700805664, value loss = 0.9342093467712402\n",
      "INFO 2024-11-30 11:57:45 pipeline.py:712] Training Step 260: Policy loss = 4.162258625030518, value loss = 0.9376485347747803\n",
      "INFO 2024-11-30 11:58:38 pipeline.py:712] Training Step 280: Policy loss = 4.150420188903809, value loss = 0.9320645332336426\n",
      "INFO 2024-11-30 11:59:32 pipeline.py:712] Training Step 300: Policy loss = 4.130703926086426, value loss = 0.9507906436920166\n",
      "INFO 2024-11-30 12:00:25 pipeline.py:712] Training Step 320: Policy loss = 4.119417190551758, value loss = 0.9516234993934631\n",
      "INFO 2024-11-30 12:01:19 pipeline.py:712] Training Step 340: Policy loss = 4.098479747772217, value loss = 0.9447280168533325\n",
      "INFO 2024-11-30 12:02:13 pipeline.py:712] Training Step 360: Policy loss = 4.119642734527588, value loss = 0.9210371375083923\n",
      "INFO 2024-11-30 12:03:06 pipeline.py:712] Training Step 380: Policy loss = 4.090270519256592, value loss = 0.9317240118980408\n",
      "INFO 2024-11-30 12:04:00 pipeline.py:712] Training Step 400: Policy loss = 4.072375297546387, value loss = 0.9211198091506958\n",
      "INFO 2024-11-30 12:04:53 pipeline.py:712] Training Step 420: Policy loss = 4.093005657196045, value loss = 0.939950704574585\n",
      "INFO 2024-11-30 12:05:47 pipeline.py:712] Training Step 440: Policy loss = 4.078087329864502, value loss = 0.919722318649292\n",
      "INFO 2024-11-30 12:06:40 pipeline.py:712] Training Step 460: Policy loss = 4.095531940460205, value loss = 0.9332715272903442\n",
      "INFO 2024-11-30 12:07:34 pipeline.py:712] Training Step 480: Policy loss = 4.070159912109375, value loss = 0.9428504705429077\n",
      "INFO 2024-11-30 12:08:37 pipeline.py:738] training_steps 484: Validation loss: Poliy loss 4.06070737369725, value_loss 0.9346269799060509\n",
      "INFO 2024-11-30 12:10:22 pipeline.py:712] Training Step 500: Policy loss = 4.090290069580078, value loss = 0.9357357025146484\n",
      "INFO 2024-11-30 12:11:50 pipeline.py:712] Training Step 520: Policy loss = 4.052045822143555, value loss = 0.9317864775657654\n",
      "INFO 2024-11-30 12:13:18 pipeline.py:712] Training Step 540: Policy loss = 4.020270347595215, value loss = 0.9194747805595398\n",
      "INFO 2024-11-30 12:14:47 pipeline.py:712] Training Step 560: Policy loss = 4.016867637634277, value loss = 0.921821117401123\n",
      "INFO 2024-11-30 12:16:15 pipeline.py:712] Training Step 580: Policy loss = 4.031061172485352, value loss = 0.9235848188400269\n",
      "INFO 2024-11-30 12:17:43 pipeline.py:712] Training Step 600: Policy loss = 4.064385414123535, value loss = 0.9373357892036438\n",
      "INFO 2024-11-30 12:19:12 pipeline.py:712] Training Step 620: Policy loss = 3.9638137817382812, value loss = 0.9338319301605225\n",
      "INFO 2024-11-30 12:20:40 pipeline.py:712] Training Step 640: Policy loss = 3.9314022064208984, value loss = 0.9365304708480835\n",
      "INFO 2024-11-30 12:22:08 pipeline.py:712] Training Step 660: Policy loss = 4.022263050079346, value loss = 0.9436007738113403\n",
      "INFO 2024-11-30 12:23:37 pipeline.py:712] Training Step 680: Policy loss = 3.9452309608459473, value loss = 0.9429512023925781\n",
      "INFO 2024-11-30 12:25:05 pipeline.py:712] Training Step 700: Policy loss = 3.903074264526367, value loss = 0.9462351202964783\n",
      "INFO 2024-11-30 12:26:33 pipeline.py:712] Training Step 720: Policy loss = 3.9495320320129395, value loss = 0.9140464067459106\n",
      "INFO 2024-11-30 12:28:01 pipeline.py:712] Training Step 740: Policy loss = 3.9136040210723877, value loss = 0.925169825553894\n",
      "INFO 2024-11-30 12:29:29 pipeline.py:712] Training Step 760: Policy loss = 3.8743247985839844, value loss = 0.9258149862289429\n",
      "INFO 2024-11-30 12:30:58 pipeline.py:712] Training Step 780: Policy loss = 3.918728828430176, value loss = 0.9319950938224792\n",
      "INFO 2024-11-30 12:32:27 pipeline.py:712] Training Step 800: Policy loss = 3.918097496032715, value loss = 0.940801203250885\n",
      "INFO 2024-11-30 12:33:55 pipeline.py:712] Training Step 820: Policy loss = 3.8727850914001465, value loss = 0.922691822052002\n",
      "INFO 2024-11-30 12:35:23 pipeline.py:712] Training Step 840: Policy loss = 3.92006778717041, value loss = 0.914993166923523\n",
      "INFO 2024-11-30 12:36:51 pipeline.py:712] Training Step 860: Policy loss = 3.851097583770752, value loss = 0.9264907240867615\n",
      "INFO 2024-11-30 12:38:20 pipeline.py:712] Training Step 880: Policy loss = 3.837622880935669, value loss = 0.9391130805015564\n",
      "INFO 2024-11-30 12:39:48 pipeline.py:712] Training Step 900: Policy loss = 3.818317413330078, value loss = 0.9292336106300354\n",
      "INFO 2024-11-30 12:41:16 pipeline.py:712] Training Step 920: Policy loss = 3.7411551475524902, value loss = 0.9195001125335693\n",
      "INFO 2024-11-30 12:42:44 pipeline.py:712] Training Step 940: Policy loss = 3.738192558288574, value loss = 0.9120282530784607\n",
      "INFO 2024-11-30 12:44:12 pipeline.py:712] Training Step 960: Policy loss = 3.734248638153076, value loss = 0.9183818101882935\n",
      "INFO 2024-11-30 12:45:20 pipeline.py:738] training_steps 968: Validation loss: Poliy loss 3.7432143199639243, value_loss 0.9241076722496846\n",
      "INFO 2024-11-30 12:46:13 pipeline.py:712] Training Step 980: Policy loss = 3.723902702331543, value loss = 0.9271118640899658\n",
      "INFO 2024-11-30 12:47:41 pipeline.py:712] Training Step 1000: Policy loss = 3.7079696655273438, value loss = 0.9070858359336853\n",
      "INFO 2024-11-30 12:49:10 pipeline.py:712] Training Step 1020: Policy loss = 3.7147841453552246, value loss = 0.9118894338607788\n",
      "INFO 2024-11-30 12:50:40 pipeline.py:712] Training Step 1040: Policy loss = 3.616180658340454, value loss = 0.9196816682815552\n",
      "INFO 2024-11-30 12:52:10 pipeline.py:712] Training Step 1060: Policy loss = 3.647432327270508, value loss = 0.9138088822364807\n",
      "INFO 2024-11-30 12:53:39 pipeline.py:712] Training Step 1080: Policy loss = 3.5767016410827637, value loss = 0.9213126301765442\n",
      "INFO 2024-11-30 12:55:08 pipeline.py:712] Training Step 1100: Policy loss = 3.5830764770507812, value loss = 0.8937044739723206\n",
      "INFO 2024-11-30 12:56:37 pipeline.py:712] Training Step 1120: Policy loss = 3.5404610633850098, value loss = 0.9094946384429932\n",
      "INFO 2024-11-30 12:58:07 pipeline.py:712] Training Step 1140: Policy loss = 3.5322718620300293, value loss = 0.9322705268859863\n",
      "INFO 2024-11-30 12:59:36 pipeline.py:712] Training Step 1160: Policy loss = 3.469446897506714, value loss = 0.9196606278419495\n",
      "INFO 2024-11-30 13:01:05 pipeline.py:712] Training Step 1180: Policy loss = 3.547755718231201, value loss = 0.9085280895233154\n",
      "INFO 2024-11-30 13:02:34 pipeline.py:712] Training Step 1200: Policy loss = 3.423351526260376, value loss = 0.9194146990776062\n",
      "INFO 2024-11-30 13:04:03 pipeline.py:712] Training Step 1220: Policy loss = 3.3659157752990723, value loss = 0.9077504277229309\n",
      "INFO 2024-11-30 13:05:33 pipeline.py:712] Training Step 1240: Policy loss = 3.3921196460723877, value loss = 0.9080178737640381\n",
      "INFO 2024-11-30 13:07:03 pipeline.py:712] Training Step 1260: Policy loss = 3.3560633659362793, value loss = 0.9158170223236084\n",
      "INFO 2024-11-30 13:08:32 pipeline.py:712] Training Step 1280: Policy loss = 3.2917559146881104, value loss = 0.9038221836090088\n",
      "INFO 2024-11-30 13:10:01 pipeline.py:712] Training Step 1300: Policy loss = 3.286862850189209, value loss = 0.8974424004554749\n",
      "INFO 2024-11-30 13:11:30 pipeline.py:712] Training Step 1320: Policy loss = 3.128652334213257, value loss = 0.9135593771934509\n",
      "INFO 2024-11-30 13:13:00 pipeline.py:712] Training Step 1340: Policy loss = 3.185521364212036, value loss = 0.8877885341644287\n",
      "INFO 2024-11-30 13:14:29 pipeline.py:712] Training Step 1360: Policy loss = 3.211367130279541, value loss = 0.900096595287323\n",
      "INFO 2024-11-30 13:15:59 pipeline.py:712] Training Step 1380: Policy loss = 3.1565589904785156, value loss = 0.933131217956543\n",
      "INFO 2024-11-30 13:17:28 pipeline.py:712] Training Step 1400: Policy loss = 3.119556427001953, value loss = 0.9081798791885376\n",
      "INFO 2024-11-30 13:18:57 pipeline.py:712] Training Step 1420: Policy loss = 3.0500876903533936, value loss = 0.87763911485672\n",
      "INFO 2024-11-30 13:20:26 pipeline.py:712] Training Step 1440: Policy loss = 2.9553661346435547, value loss = 0.911996603012085\n",
      "INFO 2024-11-30 13:21:52 pipeline.py:738] training_steps 1452: Validation loss: Poliy loss 3.0195216096815516, value_loss 0.8946968893535802\n",
      "INFO 2024-11-30 13:22:27 pipeline.py:712] Training Step 1460: Policy loss = 2.919163703918457, value loss = 0.9023799300193787\n",
      "INFO 2024-11-30 13:23:57 pipeline.py:712] Training Step 1480: Policy loss = 2.9861185550689697, value loss = 0.8945376873016357\n",
      "INFO 2024-11-30 13:25:26 pipeline.py:712] Training Step 1500: Policy loss = 2.9031119346618652, value loss = 0.8752328157424927\n",
      "INFO 2024-11-30 13:26:55 pipeline.py:712] Training Step 1520: Policy loss = 2.94968318939209, value loss = 0.890784502029419\n",
      "INFO 2024-11-30 13:28:25 pipeline.py:712] Training Step 1540: Policy loss = 2.8774943351745605, value loss = 0.8917476534843445\n",
      "INFO 2024-11-30 13:29:54 pipeline.py:712] Training Step 1560: Policy loss = 2.81795597076416, value loss = 0.8725148439407349\n",
      "INFO 2024-11-30 13:31:23 pipeline.py:712] Training Step 1580: Policy loss = 2.8611528873443604, value loss = 0.9375342130661011\n",
      "INFO 2024-11-30 13:32:52 pipeline.py:712] Training Step 1600: Policy loss = 2.8240456581115723, value loss = 0.8915659189224243\n",
      "INFO 2024-11-30 13:34:21 pipeline.py:712] Training Step 1620: Policy loss = 2.7700695991516113, value loss = 0.9130244255065918\n",
      "INFO 2024-11-30 13:35:50 pipeline.py:712] Training Step 1640: Policy loss = 2.7939984798431396, value loss = 0.8637986183166504\n",
      "INFO 2024-11-30 13:37:19 pipeline.py:712] Training Step 1660: Policy loss = 2.7484683990478516, value loss = 0.8506179451942444\n",
      "INFO 2024-11-30 13:38:48 pipeline.py:712] Training Step 1680: Policy loss = 2.6732845306396484, value loss = 0.8968189358711243\n",
      "INFO 2024-11-30 13:40:18 pipeline.py:712] Training Step 1700: Policy loss = 2.753187417984009, value loss = 0.8809047937393188\n",
      "INFO 2024-11-30 13:41:47 pipeline.py:712] Training Step 1720: Policy loss = 2.8006742000579834, value loss = 0.8608363270759583\n",
      "INFO 2024-11-30 13:43:17 pipeline.py:712] Training Step 1740: Policy loss = 2.598905086517334, value loss = 0.8693283796310425\n",
      "INFO 2024-11-30 13:44:46 pipeline.py:712] Training Step 1760: Policy loss = 2.523263931274414, value loss = 0.8801895380020142\n",
      "INFO 2024-11-30 13:46:15 pipeline.py:712] Training Step 1780: Policy loss = 2.6340994834899902, value loss = 0.8916386961936951\n",
      "INFO 2024-11-30 13:47:44 pipeline.py:712] Training Step 1800: Policy loss = 2.608220100402832, value loss = 0.8624308109283447\n",
      "INFO 2024-11-30 13:49:13 pipeline.py:712] Training Step 1820: Policy loss = 2.5642898082733154, value loss = 0.8769552707672119\n",
      "INFO 2024-11-30 13:50:42 pipeline.py:712] Training Step 1840: Policy loss = 2.595153331756592, value loss = 0.8751523494720459\n",
      "INFO 2024-11-30 13:52:11 pipeline.py:712] Training Step 1860: Policy loss = 2.522406578063965, value loss = 0.9048306941986084\n",
      "INFO 2024-11-30 13:53:40 pipeline.py:712] Training Step 1880: Policy loss = 2.5670857429504395, value loss = 0.8556356430053711\n",
      "INFO 2024-11-30 13:55:10 pipeline.py:712] Training Step 1900: Policy loss = 2.6085023880004883, value loss = 0.8728077411651611\n",
      "INFO 2024-11-30 13:56:39 pipeline.py:712] Training Step 1920: Policy loss = 2.5046966075897217, value loss = 0.8553006649017334\n",
      "INFO 2024-11-30 13:58:23 pipeline.py:738] training_steps 1936: Validation loss: Poliy loss 2.502363449237386, value_loss 0.8614587510218386\n",
      "INFO 2024-11-30 13:58:40 pipeline.py:712] Training Step 1940: Policy loss = 2.448476791381836, value loss = 0.8695511221885681\n",
      "INFO 2024-11-30 14:00:10 pipeline.py:712] Training Step 1960: Policy loss = 2.4588451385498047, value loss = 0.8559014797210693\n",
      "INFO 2024-11-30 14:01:39 pipeline.py:712] Training Step 1980: Policy loss = 2.4172797203063965, value loss = 0.8491461277008057\n",
      "INFO 2024-11-30 14:03:08 pipeline.py:712] Training Step 2000: Policy loss = 2.4463889598846436, value loss = 0.8326382637023926\n",
      "INFO 2024-11-30 14:04:37 pipeline.py:712] Training Step 2020: Policy loss = 2.358462333679199, value loss = 0.8448143005371094\n",
      "INFO 2024-11-30 14:06:07 pipeline.py:712] Training Step 2040: Policy loss = 2.3731517791748047, value loss = 0.9087396264076233\n",
      "INFO 2024-11-30 14:07:36 pipeline.py:712] Training Step 2060: Policy loss = 2.438542366027832, value loss = 0.8365290760993958\n",
      "INFO 2024-11-30 14:09:06 pipeline.py:712] Training Step 2080: Policy loss = 2.3817942142486572, value loss = 0.8437907099723816\n",
      "INFO 2024-11-30 14:10:35 pipeline.py:712] Training Step 2100: Policy loss = 2.37125301361084, value loss = 0.8110272884368896\n",
      "INFO 2024-11-30 14:12:04 pipeline.py:712] Training Step 2120: Policy loss = 2.3034181594848633, value loss = 0.8526579141616821\n",
      "INFO 2024-11-30 14:13:34 pipeline.py:712] Training Step 2140: Policy loss = 2.4282801151275635, value loss = 0.8670471906661987\n",
      "INFO 2024-11-30 14:15:03 pipeline.py:712] Training Step 2160: Policy loss = 2.3669304847717285, value loss = 0.8508645296096802\n",
      "INFO 2024-11-30 14:16:32 pipeline.py:712] Training Step 2180: Policy loss = 2.313861846923828, value loss = 0.8217120170593262\n",
      "INFO 2024-11-30 14:18:01 pipeline.py:712] Training Step 2200: Policy loss = 2.303297996520996, value loss = 0.8221209049224854\n",
      "INFO 2024-11-30 14:19:30 pipeline.py:712] Training Step 2220: Policy loss = 2.372763156890869, value loss = 0.8886971473693848\n",
      "INFO 2024-11-30 14:20:59 pipeline.py:712] Training Step 2240: Policy loss = 2.298586368560791, value loss = 0.836608350276947\n",
      "INFO 2024-11-30 14:22:29 pipeline.py:712] Training Step 2260: Policy loss = 2.2703676223754883, value loss = 0.8269782066345215\n",
      "INFO 2024-11-30 14:23:58 pipeline.py:712] Training Step 2280: Policy loss = 2.3380823135375977, value loss = 0.8626154065132141\n",
      "INFO 2024-11-30 14:25:27 pipeline.py:712] Training Step 2300: Policy loss = 2.2098469734191895, value loss = 0.8230416774749756\n",
      "INFO 2024-11-30 14:26:57 pipeline.py:712] Training Step 2320: Policy loss = 2.1346702575683594, value loss = 0.8750856518745422\n",
      "INFO 2024-11-30 14:28:26 pipeline.py:712] Training Step 2340: Policy loss = 2.3697967529296875, value loss = 0.837059736251831\n",
      "INFO 2024-11-30 14:29:55 pipeline.py:712] Training Step 2360: Policy loss = 2.3172805309295654, value loss = 0.8164286613464355\n",
      "INFO 2024-11-30 14:31:24 pipeline.py:712] Training Step 2380: Policy loss = 2.2354190349578857, value loss = 0.7910948991775513\n",
      "INFO 2024-11-30 14:32:53 pipeline.py:712] Training Step 2400: Policy loss = 2.258545398712158, value loss = 0.8408726453781128\n",
      "INFO 2024-11-30 14:34:23 pipeline.py:712] Training Step 2420: Policy loss = 2.19234561920166, value loss = 0.820418119430542\n",
      "INFO 2024-11-30 14:34:55 pipeline.py:738] training_steps 2420: Validation loss: Poliy loss 2.26999574997386, value_loss 0.8330265166329556\n",
      "INFO 2024-11-30 14:36:24 pipeline.py:712] Training Step 2440: Policy loss = 2.14837646484375, value loss = 0.808202862739563\n",
      "INFO 2024-11-30 14:37:53 pipeline.py:712] Training Step 2460: Policy loss = 2.123530864715576, value loss = 0.8052113056182861\n",
      "INFO 2024-11-30 14:39:23 pipeline.py:712] Training Step 2480: Policy loss = 2.1106529235839844, value loss = 0.8445518016815186\n",
      "INFO 2024-11-30 14:40:52 pipeline.py:712] Training Step 2500: Policy loss = 2.20175838470459, value loss = 0.8015435934066772\n",
      "INFO 2024-11-30 14:42:21 pipeline.py:712] Training Step 2520: Policy loss = 2.2192935943603516, value loss = 0.8043563365936279\n",
      "INFO 2024-11-30 14:43:50 pipeline.py:712] Training Step 2540: Policy loss = 2.1931753158569336, value loss = 0.8216323852539062\n",
      "INFO 2024-11-30 14:45:19 pipeline.py:712] Training Step 2560: Policy loss = 2.1134562492370605, value loss = 0.7962795495986938\n",
      "INFO 2024-11-30 14:46:48 pipeline.py:712] Training Step 2580: Policy loss = 2.207934856414795, value loss = 0.8077075481414795\n",
      "INFO 2024-11-30 14:48:17 pipeline.py:712] Training Step 2600: Policy loss = 2.206814765930176, value loss = 0.8547481298446655\n",
      "INFO 2024-11-30 14:49:47 pipeline.py:712] Training Step 2620: Policy loss = 2.0764336585998535, value loss = 0.8186334371566772\n",
      "INFO 2024-11-30 14:51:16 pipeline.py:712] Training Step 2640: Policy loss = 2.1852364540100098, value loss = 0.8402446508407593\n",
      "INFO 2024-11-30 14:52:45 pipeline.py:712] Training Step 2660: Policy loss = 2.165572166442871, value loss = 0.7714678645133972\n",
      "INFO 2024-11-30 14:54:15 pipeline.py:712] Training Step 2680: Policy loss = 2.202117443084717, value loss = 0.7752581834793091\n",
      "INFO 2024-11-30 14:55:44 pipeline.py:712] Training Step 2700: Policy loss = 2.203184127807617, value loss = 0.8454198241233826\n",
      "INFO 2024-11-30 14:57:13 pipeline.py:712] Training Step 2720: Policy loss = 2.167780876159668, value loss = 0.802301287651062\n",
      "INFO 2024-11-30 14:58:42 pipeline.py:712] Training Step 2740: Policy loss = 2.154810905456543, value loss = 0.8025094270706177\n",
      "INFO 2024-11-30 15:00:11 pipeline.py:712] Training Step 2760: Policy loss = 2.2083911895751953, value loss = 0.7771034836769104\n",
      "INFO 2024-11-30 15:01:40 pipeline.py:712] Training Step 2780: Policy loss = 2.1411619186401367, value loss = 0.7737735509872437\n",
      "INFO 2024-11-30 15:03:09 pipeline.py:712] Training Step 2800: Policy loss = 2.1336121559143066, value loss = 0.8034495115280151\n",
      "INFO 2024-11-30 15:04:38 pipeline.py:712] Training Step 2820: Policy loss = 2.1057536602020264, value loss = 0.7845034599304199\n",
      "INFO 2024-11-30 15:06:08 pipeline.py:712] Training Step 2840: Policy loss = 2.0535542964935303, value loss = 0.7707042098045349\n",
      "INFO 2024-11-30 15:07:37 pipeline.py:712] Training Step 2860: Policy loss = 1.9554827213287354, value loss = 0.7920041680335999\n",
      "INFO 2024-11-30 15:09:07 pipeline.py:712] Training Step 2880: Policy loss = 2.221358299255371, value loss = 0.7676944732666016\n",
      "INFO 2024-11-30 15:10:36 pipeline.py:712] Training Step 2900: Policy loss = 2.1260604858398438, value loss = 0.7989315986633301\n",
      "INFO 2024-11-30 15:11:26 pipeline.py:738] training_steps 2904: Validation loss: Poliy loss 2.1484176956239294, value_loss 0.7841446653741305\n",
      "INFO 2024-11-30 15:12:38 pipeline.py:712] Training Step 2920: Policy loss = 2.072770118713379, value loss = 0.7729847431182861\n",
      "INFO 2024-11-30 15:15:29 pipeline.py:712] Training Step 2940: Policy loss = 2.1129117012023926, value loss = 0.7622407674789429\n",
      "INFO 2024-11-30 15:18:23 pipeline.py:712] Training Step 2960: Policy loss = 1.9992341995239258, value loss = 0.752813994884491\n",
      "INFO 2024-11-30 15:21:18 pipeline.py:712] Training Step 2980: Policy loss = 2.008108377456665, value loss = 0.7543292045593262\n",
      "INFO 2024-11-30 15:24:12 pipeline.py:712] Training Step 3000: Policy loss = 2.0647430419921875, value loss = 0.7566474676132202\n",
      "INFO 2024-11-30 15:27:07 pipeline.py:712] Training Step 3020: Policy loss = 2.011630058288574, value loss = 0.7423872947692871\n",
      "INFO 2024-11-30 15:30:02 pipeline.py:712] Training Step 3040: Policy loss = 2.04697322845459, value loss = 0.8350223898887634\n",
      "INFO 2024-11-30 15:32:56 pipeline.py:712] Training Step 3060: Policy loss = 2.1028356552124023, value loss = 0.8107441067695618\n",
      "INFO 2024-11-30 15:35:50 pipeline.py:712] Training Step 3080: Policy loss = 2.0326108932495117, value loss = 0.7998514175415039\n",
      "INFO 2024-11-30 15:38:44 pipeline.py:712] Training Step 3100: Policy loss = 2.0398378372192383, value loss = 0.7513812780380249\n",
      "INFO 2024-11-30 15:41:39 pipeline.py:712] Training Step 3120: Policy loss = 2.0486881732940674, value loss = 0.7636008262634277\n",
      "INFO 2024-11-30 15:44:33 pipeline.py:712] Training Step 3140: Policy loss = 1.9911056756973267, value loss = 0.7525655031204224\n",
      "INFO 2024-11-30 15:47:27 pipeline.py:712] Training Step 3160: Policy loss = 2.0488812923431396, value loss = 0.7637917995452881\n",
      "INFO 2024-11-30 15:50:22 pipeline.py:712] Training Step 3180: Policy loss = 2.0308125019073486, value loss = 0.7548061609268188\n",
      "INFO 2024-11-30 15:53:16 pipeline.py:712] Training Step 3200: Policy loss = 2.074362277984619, value loss = 0.7412228584289551\n",
      "INFO 2024-11-30 15:56:11 pipeline.py:712] Training Step 3220: Policy loss = 2.107111692428589, value loss = 0.7951123714447021\n",
      "INFO 2024-11-30 15:59:05 pipeline.py:712] Training Step 3240: Policy loss = 2.087939977645874, value loss = 0.7183175086975098\n",
      "INFO 2024-11-30 16:02:01 pipeline.py:712] Training Step 3260: Policy loss = 2.042611598968506, value loss = 0.7581626176834106\n",
      "INFO 2024-11-30 16:04:56 pipeline.py:712] Training Step 3280: Policy loss = 2.0951900482177734, value loss = 0.7405242919921875\n",
      "INFO 2024-11-30 16:07:53 pipeline.py:712] Training Step 3300: Policy loss = 2.075045108795166, value loss = 0.739951491355896\n",
      "INFO 2024-11-30 16:10:50 pipeline.py:712] Training Step 3320: Policy loss = 2.001784563064575, value loss = 0.7478545904159546\n",
      "INFO 2024-11-30 16:13:47 pipeline.py:712] Training Step 3340: Policy loss = 1.986019253730774, value loss = 0.7086849212646484\n",
      "INFO 2024-11-30 16:16:44 pipeline.py:712] Training Step 3360: Policy loss = 2.0192830562591553, value loss = 0.7400772571563721\n",
      "INFO 2024-11-30 16:19:41 pipeline.py:712] Training Step 3380: Policy loss = 2.044157028198242, value loss = 0.7528647780418396\n",
      "INFO 2024-11-30 16:21:58 pipeline.py:738] training_steps 3388: Validation loss: Poliy loss 2.0908241897332864, value_loss 0.7479640421320181\n",
      "INFO 2024-11-30 16:23:44 pipeline.py:712] Training Step 3400: Policy loss = 1.9925293922424316, value loss = 0.6941156387329102\n",
      "INFO 2024-11-30 16:26:42 pipeline.py:712] Training Step 3420: Policy loss = 2.037212610244751, value loss = 0.6830111742019653\n",
      "INFO 2024-11-30 16:29:38 pipeline.py:712] Training Step 3440: Policy loss = 2.037874221801758, value loss = 0.7589491605758667\n",
      "INFO 2024-11-30 16:32:36 pipeline.py:712] Training Step 3460: Policy loss = 1.9955977201461792, value loss = 0.7122699022293091\n",
      "INFO 2024-11-30 16:35:34 pipeline.py:712] Training Step 3480: Policy loss = 2.000472068786621, value loss = 0.7343904972076416\n",
      "INFO 2024-11-30 16:38:33 pipeline.py:712] Training Step 3500: Policy loss = 1.9452495574951172, value loss = 0.7092848420143127\n",
      "INFO 2024-11-30 19:28:23 pipeline.py:712] Training Step 3520: Policy loss = 1.9566984176635742, value loss = 0.6726974248886108\n",
      "INFO 2024-11-30 19:30:40 pipeline.py:712] Training Step 3540: Policy loss = 1.9335952997207642, value loss = 0.7603008151054382\n",
      "INFO 2024-11-30 19:32:58 pipeline.py:712] Training Step 3560: Policy loss = 1.9690604209899902, value loss = 0.6851722002029419\n",
      "INFO 2024-11-30 19:35:24 pipeline.py:712] Training Step 3580: Policy loss = 1.9817944765090942, value loss = 0.7112632989883423\n",
      "INFO 2024-11-30 19:37:47 pipeline.py:712] Training Step 3600: Policy loss = 1.9253194332122803, value loss = 0.7165874242782593\n",
      "INFO 2024-11-30 19:40:09 pipeline.py:712] Training Step 3620: Policy loss = 1.9167152643203735, value loss = 0.6786930561065674\n",
      "INFO 2024-11-30 19:42:39 pipeline.py:712] Training Step 3640: Policy loss = 1.9163670539855957, value loss = 0.7025482058525085\n",
      "INFO 2024-11-30 19:45:04 pipeline.py:712] Training Step 3660: Policy loss = 1.9322969913482666, value loss = 0.7308692932128906\n",
      "INFO 2024-11-30 19:47:41 pipeline.py:712] Training Step 3680: Policy loss = 1.9483633041381836, value loss = 0.6672073602676392\n",
      "INFO 2024-11-30 19:50:07 pipeline.py:712] Training Step 3700: Policy loss = 2.046445369720459, value loss = 0.6941609382629395\n",
      "INFO 2024-11-30 19:52:39 pipeline.py:712] Training Step 3720: Policy loss = 1.9868162870407104, value loss = 0.6439032554626465\n",
      "INFO 2024-11-30 19:55:11 pipeline.py:712] Training Step 3740: Policy loss = 2.012263536453247, value loss = 0.6904454231262207\n",
      "INFO 2024-11-30 19:57:42 pipeline.py:712] Training Step 3760: Policy loss = 1.9550116062164307, value loss = 0.6567063927650452\n",
      "INFO 2024-11-30 20:00:07 pipeline.py:712] Training Step 3780: Policy loss = 1.9183120727539062, value loss = 0.7151731252670288\n",
      "INFO 2024-11-30 20:02:35 pipeline.py:712] Training Step 3800: Policy loss = 1.8913710117340088, value loss = 0.7103210091590881\n",
      "INFO 2024-11-30 20:04:58 pipeline.py:712] Training Step 3820: Policy loss = 1.9483609199523926, value loss = 0.669154942035675\n",
      "INFO 2024-11-30 20:07:41 pipeline.py:712] Training Step 3840: Policy loss = 1.979610562324524, value loss = 0.652496337890625\n",
      "INFO 2024-11-30 20:10:27 pipeline.py:712] Training Step 3860: Policy loss = 1.9929200410842896, value loss = 0.6404342651367188\n",
      "INFO 2024-11-30 20:12:35 pipeline.py:738] training_steps 3872: Validation loss: Poliy loss 2.0514546789106776, value_loss 0.6728587951816496\n",
      "INFO 2024-11-30 20:13:40 pipeline.py:712] Training Step 3880: Policy loss = 1.8896186351776123, value loss = 0.6203259229660034\n",
      "INFO 2024-11-30 20:16:23 pipeline.py:712] Training Step 3900: Policy loss = 1.9902957677841187, value loss = 0.627861738204956\n",
      "INFO 2024-11-30 20:19:05 pipeline.py:712] Training Step 3920: Policy loss = 1.8446694612503052, value loss = 0.6412383913993835\n",
      "INFO 2024-11-30 20:21:48 pipeline.py:712] Training Step 3940: Policy loss = 1.8940038681030273, value loss = 0.6962717771530151\n",
      "INFO 2024-11-30 20:24:32 pipeline.py:712] Training Step 3960: Policy loss = 1.929864525794983, value loss = 0.6195703744888306\n",
      "INFO 2024-11-30 20:27:16 pipeline.py:712] Training Step 3980: Policy loss = 1.912927508354187, value loss = 0.6141933798789978\n",
      "INFO 2024-11-30 20:29:57 pipeline.py:712] Training Step 4000: Policy loss = 1.9289878606796265, value loss = 0.6385825872421265\n",
      "INFO 2024-11-30 20:32:42 pipeline.py:712] Training Step 4020: Policy loss = 1.8958580493927002, value loss = 0.7253183126449585\n",
      "INFO 2024-11-30 20:35:23 pipeline.py:712] Training Step 4040: Policy loss = 1.925898551940918, value loss = 0.6533531546592712\n",
      "INFO 2024-11-30 20:38:11 pipeline.py:712] Training Step 4060: Policy loss = 1.9543352127075195, value loss = 0.5789793133735657\n",
      "INFO 2024-11-30 20:41:01 pipeline.py:712] Training Step 4080: Policy loss = 1.97198486328125, value loss = 0.620508074760437\n",
      "INFO 2024-11-30 20:43:40 pipeline.py:712] Training Step 4100: Policy loss = 1.8808032274246216, value loss = 0.5752224922180176\n",
      "INFO 2024-11-30 20:46:17 pipeline.py:712] Training Step 4120: Policy loss = 1.9289333820343018, value loss = 0.594132125377655\n",
      "INFO 2024-11-30 20:48:56 pipeline.py:712] Training Step 4140: Policy loss = 1.9844882488250732, value loss = 0.6328456997871399\n",
      "INFO 2024-11-30 20:51:33 pipeline.py:712] Training Step 4160: Policy loss = 2.0077567100524902, value loss = 0.6320722103118896\n",
      "INFO 2024-11-30 20:54:08 pipeline.py:712] Training Step 4180: Policy loss = 1.9215847253799438, value loss = 0.5927890539169312\n",
      "INFO 2024-11-30 20:56:53 pipeline.py:712] Training Step 4200: Policy loss = 1.947222352027893, value loss = 0.5536819100379944\n",
      "INFO 2024-11-30 20:59:37 pipeline.py:712] Training Step 4220: Policy loss = 1.9791923761367798, value loss = 0.6499607563018799\n",
      "INFO 2024-11-30 21:02:22 pipeline.py:712] Training Step 4240: Policy loss = 1.8446600437164307, value loss = 0.6522022485733032\n",
      "INFO 2024-11-30 21:05:04 pipeline.py:712] Training Step 4260: Policy loss = 1.9141998291015625, value loss = 0.6094781160354614\n",
      "INFO 2024-11-30 21:07:49 pipeline.py:712] Training Step 4280: Policy loss = 1.8116438388824463, value loss = 0.6147971153259277\n",
      "INFO 2024-11-30 21:10:34 pipeline.py:712] Training Step 4300: Policy loss = 1.9349188804626465, value loss = 0.5913493633270264\n",
      "INFO 2024-11-30 21:13:20 pipeline.py:712] Training Step 4320: Policy loss = 1.9979608058929443, value loss = 0.6328742504119873\n",
      "INFO 2024-11-30 21:16:04 pipeline.py:712] Training Step 4340: Policy loss = 1.9363245964050293, value loss = 0.6249715089797974\n",
      "INFO 2024-11-30 21:18:53 pipeline.py:738] training_steps 4356: Validation loss: Poliy loss 2.022727295023496, value_loss 0.6376122084797405\n",
      "INFO 2024-11-30 21:19:26 pipeline.py:712] Training Step 4360: Policy loss = 1.862131953239441, value loss = 0.49451136589050293\n",
      "INFO 2024-11-30 21:22:10 pipeline.py:712] Training Step 4380: Policy loss = 1.783246397972107, value loss = 0.5752608776092529\n",
      "INFO 2024-11-30 21:24:55 pipeline.py:712] Training Step 4400: Policy loss = 1.9072527885437012, value loss = 0.5528614521026611\n",
      "INFO 2024-11-30 21:27:37 pipeline.py:712] Training Step 4420: Policy loss = 1.9168049097061157, value loss = 0.5540447235107422\n",
      "INFO 2024-11-30 21:30:11 pipeline.py:712] Training Step 4440: Policy loss = 1.9361029863357544, value loss = 0.5180307626724243\n",
      "INFO 2024-11-30 21:32:45 pipeline.py:712] Training Step 4460: Policy loss = 2.01182222366333, value loss = 0.547017514705658\n",
      "INFO 2024-11-30 21:35:18 pipeline.py:712] Training Step 4480: Policy loss = 1.8241043090820312, value loss = 0.6051161885261536\n",
      "INFO 2024-11-30 21:37:55 pipeline.py:712] Training Step 4500: Policy loss = 1.9573525190353394, value loss = 0.5450130701065063\n",
      "INFO 2024-11-30 21:40:31 pipeline.py:712] Training Step 4520: Policy loss = 1.8769127130508423, value loss = 0.546902060508728\n",
      "INFO 2024-11-30 21:43:07 pipeline.py:712] Training Step 4540: Policy loss = 2.0074143409729004, value loss = 0.5815902948379517\n",
      "INFO 2024-11-30 21:45:43 pipeline.py:712] Training Step 4560: Policy loss = 1.8848271369934082, value loss = 0.5732619762420654\n",
      "INFO 2024-11-30 21:48:18 pipeline.py:712] Training Step 4580: Policy loss = 1.8407217264175415, value loss = 0.5831255316734314\n",
      "INFO 2024-11-30 21:50:54 pipeline.py:712] Training Step 4600: Policy loss = 1.960060715675354, value loss = 0.564638614654541\n",
      "INFO 2024-11-30 21:53:29 pipeline.py:712] Training Step 4620: Policy loss = 1.900306224822998, value loss = 0.5778693556785583\n",
      "INFO 2024-11-30 21:56:05 pipeline.py:712] Training Step 4640: Policy loss = 1.90968918800354, value loss = 0.5449192523956299\n",
      "INFO 2024-11-30 21:58:40 pipeline.py:712] Training Step 4660: Policy loss = 1.913191556930542, value loss = 0.5133693218231201\n",
      "INFO 2024-11-30 22:01:16 pipeline.py:712] Training Step 4680: Policy loss = 1.9440639019012451, value loss = 0.5566787123680115\n",
      "INFO 2024-11-30 22:03:51 pipeline.py:712] Training Step 4700: Policy loss = 1.982790231704712, value loss = 0.47600671648979187\n",
      "INFO 2024-11-30 22:06:27 pipeline.py:712] Training Step 4720: Policy loss = 1.9381580352783203, value loss = 0.5305269360542297\n",
      "INFO 2024-11-30 22:09:02 pipeline.py:712] Training Step 4740: Policy loss = 2.015329122543335, value loss = 0.5385438203811646\n",
      "INFO 2024-11-30 22:11:38 pipeline.py:712] Training Step 4760: Policy loss = 1.886478304862976, value loss = 0.5367976427078247\n",
      "INFO 2024-11-30 22:14:14 pipeline.py:712] Training Step 4780: Policy loss = 1.8932366371154785, value loss = 0.47059836983680725\n",
      "INFO 2024-11-30 22:16:49 pipeline.py:712] Training Step 4800: Policy loss = 1.9265480041503906, value loss = 0.5187596678733826\n",
      "INFO 2024-11-30 22:19:25 pipeline.py:712] Training Step 4820: Policy loss = 1.9155539274215698, value loss = 0.5088802576065063\n",
      "INFO 2024-11-30 22:22:00 pipeline.py:712] Training Step 4840: Policy loss = 1.8386297225952148, value loss = 0.5494310259819031\n",
      "INFO 2024-11-30 22:22:33 pipeline.py:738] training_steps 4840: Validation loss: Poliy loss 2.0041247811473784, value_loss 0.5669842450345148\n",
      "INFO 2024-11-30 22:25:09 pipeline.py:712] Training Step 4860: Policy loss = 1.9317446947097778, value loss = 0.45431989431381226\n",
      "INFO 2024-11-30 22:27:44 pipeline.py:712] Training Step 4880: Policy loss = 1.8705167770385742, value loss = 0.5639971494674683\n",
      "INFO 2024-11-30 22:30:20 pipeline.py:712] Training Step 4900: Policy loss = 1.8989335298538208, value loss = 0.4782537817955017\n",
      "INFO 2024-11-30 22:32:56 pipeline.py:712] Training Step 4920: Policy loss = 1.8698142766952515, value loss = 0.4491729140281677\n",
      "INFO 2024-11-30 22:35:31 pipeline.py:712] Training Step 4940: Policy loss = 1.8595585823059082, value loss = 0.495434045791626\n",
      "INFO 2024-11-30 22:38:07 pipeline.py:712] Training Step 4960: Policy loss = 1.892754077911377, value loss = 0.49080416560173035\n",
      "INFO 2024-11-30 22:40:43 pipeline.py:712] Training Step 4980: Policy loss = 1.9873642921447754, value loss = 0.4881969392299652\n",
      "INFO 2024-11-30 22:43:18 pipeline.py:712] Training Step 5000: Policy loss = 1.8686201572418213, value loss = 0.47351181507110596\n",
      "INFO 2024-11-30 22:45:53 pipeline.py:712] Training Step 5020: Policy loss = 1.8831219673156738, value loss = 0.5325614809989929\n",
      "INFO 2024-11-30 22:48:29 pipeline.py:712] Training Step 5040: Policy loss = 1.8414835929870605, value loss = 0.45897793769836426\n",
      "INFO 2024-11-30 22:51:04 pipeline.py:712] Training Step 5060: Policy loss = 1.8640670776367188, value loss = 0.45352500677108765\n",
      "INFO 2024-11-30 22:53:40 pipeline.py:712] Training Step 5080: Policy loss = 1.8583221435546875, value loss = 0.4609948396682739\n",
      "INFO 2024-11-30 22:56:17 pipeline.py:712] Training Step 5100: Policy loss = 1.8614897727966309, value loss = 0.44875890016555786\n",
      "INFO 2024-11-30 22:58:55 pipeline.py:712] Training Step 5120: Policy loss = 1.8483986854553223, value loss = 0.5195795297622681\n",
      "INFO 2024-11-30 23:01:32 pipeline.py:712] Training Step 5140: Policy loss = 1.8297685384750366, value loss = 0.5035554766654968\n",
      "INFO 2024-11-30 23:04:07 pipeline.py:712] Training Step 5160: Policy loss = 1.8627424240112305, value loss = 0.4881495237350464\n",
      "INFO 2024-11-30 23:06:42 pipeline.py:712] Training Step 5180: Policy loss = 1.911186933517456, value loss = 0.46815887093544006\n",
      "INFO 2024-11-30 23:09:17 pipeline.py:712] Training Step 5200: Policy loss = 1.8914401531219482, value loss = 0.5010240077972412\n",
      "INFO 2024-11-30 23:11:52 pipeline.py:712] Training Step 5220: Policy loss = 1.859430193901062, value loss = 0.5106495022773743\n",
      "INFO 2024-11-30 23:14:26 pipeline.py:712] Training Step 5240: Policy loss = 1.8869394063949585, value loss = 0.5031720399856567\n",
      "INFO 2024-11-30 23:17:01 pipeline.py:712] Training Step 5260: Policy loss = 1.8564553260803223, value loss = 0.46664541959762573\n",
      "INFO 2024-11-30 23:19:36 pipeline.py:712] Training Step 5280: Policy loss = 1.9361200332641602, value loss = 0.4922662377357483\n",
      "INFO 2024-11-30 23:22:11 pipeline.py:712] Training Step 5300: Policy loss = 1.9488024711608887, value loss = 0.5176039934158325\n",
      "INFO 2024-11-30 23:24:45 pipeline.py:712] Training Step 5320: Policy loss = 1.9094393253326416, value loss = 0.521354079246521\n",
      "INFO 2024-11-30 23:25:49 pipeline.py:738] training_steps 5324: Validation loss: Poliy loss 1.9876111951030668, value_loss 0.5305193384162715\n",
      "INFO 2024-11-30 23:27:53 pipeline.py:712] Training Step 5340: Policy loss = 1.785391092300415, value loss = 0.42829883098602295\n",
      "INFO 2024-11-30 23:30:28 pipeline.py:712] Training Step 5360: Policy loss = 1.8526437282562256, value loss = 0.4300728738307953\n",
      "INFO 2024-11-30 23:33:03 pipeline.py:712] Training Step 5380: Policy loss = 1.7915079593658447, value loss = 0.4154437184333801\n",
      "INFO 2024-11-30 23:35:37 pipeline.py:712] Training Step 5400: Policy loss = 1.8619366884231567, value loss = 0.48271089792251587\n",
      "INFO 2024-11-30 23:38:12 pipeline.py:712] Training Step 5420: Policy loss = 1.8027713298797607, value loss = 0.42725569009780884\n",
      "INFO 2024-11-30 23:40:47 pipeline.py:712] Training Step 5440: Policy loss = 1.766021490097046, value loss = 0.45038172602653503\n",
      "INFO 2024-11-30 23:43:21 pipeline.py:712] Training Step 5460: Policy loss = 1.909266710281372, value loss = 0.4764414429664612\n",
      "INFO 2024-11-30 23:45:56 pipeline.py:712] Training Step 5480: Policy loss = 1.854164481163025, value loss = 0.4409378170967102\n",
      "INFO 2024-11-30 23:48:31 pipeline.py:712] Training Step 5500: Policy loss = 1.7448863983154297, value loss = 0.46618860960006714\n",
      "INFO 2024-11-30 23:51:06 pipeline.py:712] Training Step 5520: Policy loss = 1.771437406539917, value loss = 0.4378640055656433\n",
      "INFO 2024-11-30 23:53:41 pipeline.py:712] Training Step 5540: Policy loss = 1.931539535522461, value loss = 0.41915035247802734\n",
      "INFO 2024-11-30 23:56:16 pipeline.py:712] Training Step 5560: Policy loss = 1.9629427194595337, value loss = 0.41086408495903015\n",
      "INFO 2024-11-30 23:58:51 pipeline.py:712] Training Step 5580: Policy loss = 1.8358492851257324, value loss = 0.4213063716888428\n",
      "INFO 2024-12-01 00:01:25 pipeline.py:712] Training Step 5600: Policy loss = 1.7998137474060059, value loss = 0.4506652355194092\n",
      "INFO 2024-12-01 00:04:00 pipeline.py:712] Training Step 5620: Policy loss = 1.8163483142852783, value loss = 0.4313403367996216\n",
      "INFO 2024-12-01 00:06:37 pipeline.py:712] Training Step 5640: Policy loss = 1.8096485137939453, value loss = 0.47116294503211975\n",
      "INFO 2024-12-01 00:09:13 pipeline.py:712] Training Step 5660: Policy loss = 1.931506872177124, value loss = 0.44021156430244446\n",
      "INFO 2024-12-01 00:11:49 pipeline.py:712] Training Step 5680: Policy loss = 1.8532776832580566, value loss = 0.4651305079460144\n",
      "INFO 2024-12-01 00:14:25 pipeline.py:712] Training Step 5700: Policy loss = 1.8003406524658203, value loss = 0.4525953531265259\n",
      "INFO 2024-12-01 00:17:01 pipeline.py:712] Training Step 5720: Policy loss = 1.8244001865386963, value loss = 0.41700875759124756\n",
      "INFO 2024-12-01 00:19:37 pipeline.py:712] Training Step 5740: Policy loss = 1.8189818859100342, value loss = 0.4039371609687805\n",
      "INFO 2024-12-01 00:22:14 pipeline.py:712] Training Step 5760: Policy loss = 1.8123688697814941, value loss = 0.4642959237098694\n",
      "INFO 2024-12-01 00:24:50 pipeline.py:712] Training Step 5780: Policy loss = 1.8300036191940308, value loss = 0.43416303396224976\n",
      "INFO 2024-12-01 00:27:26 pipeline.py:712] Training Step 5800: Policy loss = 1.9081498384475708, value loss = 0.41785940527915955\n",
      "INFO 2024-12-01 00:29:02 pipeline.py:738] training_steps 5808: Validation loss: Poliy loss 1.9699883040834645, value_loss 0.5157189220190048\n",
      "INFO 2024-12-01 00:30:35 pipeline.py:712] Training Step 5820: Policy loss = 1.8185646533966064, value loss = 0.3771754503250122\n",
      "INFO 2024-12-01 00:33:12 pipeline.py:712] Training Step 5840: Policy loss = 1.7484498023986816, value loss = 0.3931000828742981\n",
      "INFO 2024-12-01 00:35:48 pipeline.py:712] Training Step 5860: Policy loss = 1.7915232181549072, value loss = 0.4264875650405884\n",
      "INFO 2024-12-01 00:38:24 pipeline.py:712] Training Step 5880: Policy loss = 1.7470424175262451, value loss = 0.39455753564834595\n",
      "INFO 2024-12-01 00:41:00 pipeline.py:712] Training Step 5900: Policy loss = 1.7732064723968506, value loss = 0.3842635154724121\n",
      "INFO 2024-12-01 00:43:35 pipeline.py:712] Training Step 5920: Policy loss = 1.8027915954589844, value loss = 0.3795885443687439\n",
      "INFO 2024-12-01 00:46:12 pipeline.py:712] Training Step 5940: Policy loss = 1.7187535762786865, value loss = 0.4465552568435669\n",
      "INFO 2024-12-01 00:48:48 pipeline.py:712] Training Step 5960: Policy loss = 1.7484005689620972, value loss = 0.42798253893852234\n",
      "INFO 2024-12-01 00:51:24 pipeline.py:712] Training Step 5980: Policy loss = 1.808565378189087, value loss = 0.38677024841308594\n",
      "INFO 2024-12-01 00:54:00 pipeline.py:712] Training Step 6000: Policy loss = 1.7695577144622803, value loss = 0.40355125069618225\n",
      "INFO 2024-12-01 00:56:36 pipeline.py:712] Training Step 6020: Policy loss = 1.8564941883087158, value loss = 0.4242898225784302\n",
      "INFO 2024-12-01 00:59:11 pipeline.py:712] Training Step 6040: Policy loss = 1.766355037689209, value loss = 0.3850364685058594\n",
      "INFO 2024-12-01 01:01:47 pipeline.py:712] Training Step 6060: Policy loss = 1.877185344696045, value loss = 0.3939902186393738\n",
      "INFO 2024-12-01 01:04:24 pipeline.py:712] Training Step 6080: Policy loss = 1.883995532989502, value loss = 0.4237626791000366\n",
      "INFO 2024-12-01 01:07:00 pipeline.py:712] Training Step 6100: Policy loss = 1.8171405792236328, value loss = 0.37677523493766785\n",
      "INFO 2024-12-01 01:09:36 pipeline.py:712] Training Step 6120: Policy loss = 1.8304260969161987, value loss = 0.3837672173976898\n",
      "INFO 2024-12-01 01:12:12 pipeline.py:712] Training Step 6140: Policy loss = 1.762691855430603, value loss = 0.3958870768547058\n",
      "INFO 2024-12-01 01:14:48 pipeline.py:712] Training Step 6160: Policy loss = 1.794003963470459, value loss = 0.3862038254737854\n",
      "INFO 2024-12-01 01:17:24 pipeline.py:712] Training Step 6180: Policy loss = 1.7885056734085083, value loss = 0.4133337140083313\n",
      "INFO 2024-12-01 01:20:00 pipeline.py:712] Training Step 6200: Policy loss = 1.8431169986724854, value loss = 0.385454922914505\n",
      "INFO 2024-12-01 01:22:37 pipeline.py:712] Training Step 6220: Policy loss = 1.8154103755950928, value loss = 0.3979341983795166\n",
      "INFO 2024-12-01 01:25:13 pipeline.py:712] Training Step 6240: Policy loss = 1.8234455585479736, value loss = 0.4110119342803955\n",
      "INFO 2024-12-01 01:27:49 pipeline.py:712] Training Step 6260: Policy loss = 1.8161674737930298, value loss = 0.3935253620147705\n",
      "INFO 2024-12-01 01:30:25 pipeline.py:712] Training Step 6280: Policy loss = 1.823143482208252, value loss = 0.3591901957988739\n",
      "INFO 2024-12-01 01:32:32 pipeline.py:738] training_steps 6292: Validation loss: Poliy loss 1.9509560241073858, value_loss 0.4894905764548505\n",
      "INFO 2024-12-01 01:33:34 pipeline.py:712] Training Step 6300: Policy loss = 1.7817137241363525, value loss = 0.3395817279815674\n",
      "INFO 2024-12-01 01:36:10 pipeline.py:712] Training Step 6320: Policy loss = 1.6369409561157227, value loss = 0.35862693190574646\n",
      "INFO 2024-12-01 01:38:46 pipeline.py:712] Training Step 6340: Policy loss = 1.829702615737915, value loss = 0.36250442266464233\n",
      "INFO 2024-12-01 01:41:23 pipeline.py:712] Training Step 6360: Policy loss = 1.7867496013641357, value loss = 0.3741127848625183\n",
      "INFO 2024-12-01 01:43:58 pipeline.py:712] Training Step 6380: Policy loss = 1.7813795804977417, value loss = 0.36835458874702454\n",
      "INFO 2024-12-01 01:46:34 pipeline.py:712] Training Step 6400: Policy loss = 1.7894394397735596, value loss = 0.3771502375602722\n",
      "INFO 2024-12-01 01:49:10 pipeline.py:712] Training Step 6420: Policy loss = 1.7011452913284302, value loss = 0.36502039432525635\n",
      "INFO 2024-12-01 01:51:47 pipeline.py:712] Training Step 6440: Policy loss = 1.6402324438095093, value loss = 0.3465576767921448\n",
      "INFO 2024-12-01 01:54:23 pipeline.py:712] Training Step 6460: Policy loss = 1.7070773839950562, value loss = 0.3678039312362671\n",
      "INFO 2024-12-01 01:56:59 pipeline.py:712] Training Step 6480: Policy loss = 1.7861355543136597, value loss = 0.3907596170902252\n",
      "INFO 2024-12-01 01:59:35 pipeline.py:712] Training Step 6500: Policy loss = 1.7446925640106201, value loss = 0.4164789617061615\n",
      "INFO 2024-12-01 02:02:11 pipeline.py:712] Training Step 6520: Policy loss = 1.7225263118743896, value loss = 0.37463903427124023\n",
      "INFO 2024-12-01 02:04:47 pipeline.py:712] Training Step 6540: Policy loss = 1.7553136348724365, value loss = 0.3805009126663208\n",
      "INFO 2024-12-01 02:07:23 pipeline.py:712] Training Step 6560: Policy loss = 1.786590814590454, value loss = 0.4113750457763672\n",
      "INFO 2024-12-01 02:09:59 pipeline.py:712] Training Step 6580: Policy loss = 1.820683240890503, value loss = 0.3665577173233032\n",
      "INFO 2024-12-01 02:12:35 pipeline.py:712] Training Step 6600: Policy loss = 1.7643046379089355, value loss = 0.36234164237976074\n",
      "INFO 2024-12-01 02:15:11 pipeline.py:712] Training Step 6620: Policy loss = 1.7773020267486572, value loss = 0.3148752450942993\n",
      "INFO 2024-12-01 02:17:47 pipeline.py:712] Training Step 6640: Policy loss = 1.8119075298309326, value loss = 0.3294897675514221\n",
      "INFO 2024-12-01 02:20:23 pipeline.py:712] Training Step 6660: Policy loss = 1.8086148500442505, value loss = 0.3922385573387146\n",
      "INFO 2024-12-01 02:23:00 pipeline.py:712] Training Step 6680: Policy loss = 1.7696802616119385, value loss = 0.3663647174835205\n",
      "INFO 2024-12-01 02:25:36 pipeline.py:712] Training Step 6700: Policy loss = 1.8028621673583984, value loss = 0.36510688066482544\n",
      "INFO 2024-12-01 02:28:12 pipeline.py:712] Training Step 6720: Policy loss = 1.7925140857696533, value loss = 0.32400691509246826\n",
      "INFO 2024-12-01 02:30:48 pipeline.py:712] Training Step 6740: Policy loss = 1.7766972780227661, value loss = 0.34228652715682983\n",
      "INFO 2024-12-01 02:33:24 pipeline.py:712] Training Step 6760: Policy loss = 1.6916720867156982, value loss = 0.40042468905448914\n",
      "INFO 2024-12-01 02:36:02 pipeline.py:738] training_steps 6776: Validation loss: Poliy loss 1.9681414443938459, value_loss 0.47179025826884097\n",
      "INFO 2024-12-01 02:36:34 pipeline.py:712] Training Step 6780: Policy loss = 1.7378618717193604, value loss = 0.3330781161785126\n",
      "INFO 2024-12-01 02:39:10 pipeline.py:712] Training Step 6800: Policy loss = 1.7296040058135986, value loss = 0.3514927625656128\n",
      "INFO 2024-12-01 02:41:46 pipeline.py:712] Training Step 6820: Policy loss = 1.640590786933899, value loss = 0.2919766902923584\n",
      "INFO 2024-12-01 02:44:21 pipeline.py:712] Training Step 6840: Policy loss = 1.6317895650863647, value loss = 0.31425178050994873\n",
      "INFO 2024-12-01 02:46:57 pipeline.py:712] Training Step 6860: Policy loss = 1.8606764078140259, value loss = 0.37878596782684326\n",
      "INFO 2024-12-01 02:49:33 pipeline.py:712] Training Step 6880: Policy loss = 1.7826859951019287, value loss = 0.3176077604293823\n",
      "INFO 2024-12-01 02:52:09 pipeline.py:712] Training Step 6900: Policy loss = 1.748770833015442, value loss = 0.31982195377349854\n",
      "INFO 2024-12-01 02:54:45 pipeline.py:712] Training Step 6920: Policy loss = 1.7762078046798706, value loss = 0.33834341168403625\n",
      "INFO 2024-12-01 02:57:21 pipeline.py:712] Training Step 6940: Policy loss = 1.8090803623199463, value loss = 0.3457968831062317\n",
      "INFO 2024-12-01 02:59:57 pipeline.py:712] Training Step 6960: Policy loss = 1.722312331199646, value loss = 0.34368225932121277\n",
      "INFO 2024-12-01 03:02:33 pipeline.py:712] Training Step 6980: Policy loss = 1.773858666419983, value loss = 0.3399878740310669\n",
      "INFO 2024-12-01 03:05:09 pipeline.py:712] Training Step 7000: Policy loss = 1.6592741012573242, value loss = 0.37764042615890503\n",
      "INFO 2024-12-01 03:07:46 pipeline.py:712] Training Step 7020: Policy loss = 1.8038856983184814, value loss = 0.3660263121128082\n",
      "INFO 2024-12-01 03:10:22 pipeline.py:712] Training Step 7040: Policy loss = 1.7246506214141846, value loss = 0.3458395004272461\n",
      "INFO 2024-12-01 03:12:58 pipeline.py:712] Training Step 7060: Policy loss = 1.7474443912506104, value loss = 0.3334695100784302\n",
      "INFO 2024-12-01 03:15:34 pipeline.py:712] Training Step 7080: Policy loss = 1.775784969329834, value loss = 0.33032703399658203\n",
      "INFO 2024-12-01 03:18:10 pipeline.py:712] Training Step 7100: Policy loss = 1.753971815109253, value loss = 0.34428128600120544\n",
      "INFO 2024-12-01 03:20:47 pipeline.py:712] Training Step 7120: Policy loss = 1.7354562282562256, value loss = 0.3721393942832947\n",
      "INFO 2024-12-01 03:23:23 pipeline.py:712] Training Step 7140: Policy loss = 1.7708730697631836, value loss = 0.33288687467575073\n",
      "INFO 2024-12-01 03:25:59 pipeline.py:712] Training Step 7160: Policy loss = 1.7799217700958252, value loss = 0.3483394980430603\n",
      "INFO 2024-12-01 03:28:35 pipeline.py:712] Training Step 7180: Policy loss = 1.7771027088165283, value loss = 0.3686477541923523\n",
      "INFO 2024-12-01 03:31:10 pipeline.py:712] Training Step 7200: Policy loss = 1.7081871032714844, value loss = 0.3476802110671997\n",
      "INFO 2024-12-01 03:33:46 pipeline.py:712] Training Step 7220: Policy loss = 1.7805136442184448, value loss = 0.39664575457572937\n",
      "INFO 2024-12-01 03:36:22 pipeline.py:712] Training Step 7240: Policy loss = 1.7226433753967285, value loss = 0.3782692551612854\n",
      "INFO 2024-12-01 03:38:59 pipeline.py:712] Training Step 7260: Policy loss = 1.7548409700393677, value loss = 0.3897286355495453\n",
      "INFO 2024-12-01 03:39:32 pipeline.py:738] training_steps 7260: Validation loss: Poliy loss 1.9217276426612353, value_loss 0.4348265277069123\n",
      "INFO 2024-12-01 03:42:08 pipeline.py:712] Training Step 7280: Policy loss = 1.771531343460083, value loss = 0.3652765154838562\n",
      "INFO 2024-12-01 03:44:43 pipeline.py:712] Training Step 7300: Policy loss = 1.7397475242614746, value loss = 0.30233633518218994\n",
      "INFO 2024-12-01 03:47:19 pipeline.py:712] Training Step 7320: Policy loss = 1.7171615362167358, value loss = 0.334365576505661\n",
      "INFO 2024-12-01 03:49:55 pipeline.py:712] Training Step 7340: Policy loss = 1.727186679840088, value loss = 0.3170582950115204\n",
      "INFO 2024-12-01 03:52:31 pipeline.py:712] Training Step 7360: Policy loss = 1.7612478733062744, value loss = 0.3407224118709564\n",
      "INFO 2024-12-01 03:55:08 pipeline.py:712] Training Step 7380: Policy loss = 1.7265446186065674, value loss = 0.3675621449947357\n",
      "INFO 2024-12-01 03:57:44 pipeline.py:712] Training Step 7400: Policy loss = 1.7718912363052368, value loss = 0.32636797428131104\n",
      "INFO 2024-12-01 04:00:19 pipeline.py:712] Training Step 7420: Policy loss = 1.8064855337142944, value loss = 0.31282445788383484\n",
      "INFO 2024-12-01 04:02:55 pipeline.py:712] Training Step 7440: Policy loss = 1.751771092414856, value loss = 0.31096315383911133\n",
      "INFO 2024-12-01 04:05:31 pipeline.py:712] Training Step 7460: Policy loss = 1.7423484325408936, value loss = 0.31455111503601074\n",
      "INFO 2024-12-01 04:08:07 pipeline.py:712] Training Step 7480: Policy loss = 1.7547810077667236, value loss = 0.34307485818862915\n",
      "INFO 2024-12-01 04:10:43 pipeline.py:712] Training Step 7500: Policy loss = 1.74173903465271, value loss = 0.31861621141433716\n",
      "INFO 2024-12-01 04:13:19 pipeline.py:712] Training Step 7520: Policy loss = 1.6820802688598633, value loss = 0.2992176413536072\n",
      "INFO 2024-12-01 04:15:55 pipeline.py:712] Training Step 7540: Policy loss = 1.7153956890106201, value loss = 0.3080778419971466\n",
      "INFO 2024-12-01 04:18:31 pipeline.py:712] Training Step 7560: Policy loss = 1.7143850326538086, value loss = 0.2930029332637787\n",
      "INFO 2024-12-01 04:21:08 pipeline.py:712] Training Step 7580: Policy loss = 1.7238428592681885, value loss = 0.31704288721084595\n",
      "INFO 2024-12-01 04:23:44 pipeline.py:712] Training Step 7600: Policy loss = 1.784853219985962, value loss = 0.31721681356430054\n",
      "INFO 2024-12-01 04:26:21 pipeline.py:712] Training Step 7620: Policy loss = 1.695759654045105, value loss = 0.2994741201400757\n",
      "INFO 2024-12-01 04:28:57 pipeline.py:712] Training Step 7640: Policy loss = 1.6922688484191895, value loss = 0.31644880771636963\n",
      "INFO 2024-12-01 04:31:33 pipeline.py:712] Training Step 7660: Policy loss = 1.760224461555481, value loss = 0.329894483089447\n",
      "INFO 2024-12-01 04:34:09 pipeline.py:712] Training Step 7680: Policy loss = 1.7362337112426758, value loss = 0.30678170919418335\n",
      "INFO 2024-12-01 04:36:45 pipeline.py:712] Training Step 7700: Policy loss = 1.7666480541229248, value loss = 0.33619922399520874\n",
      "INFO 2024-12-01 04:39:21 pipeline.py:712] Training Step 7720: Policy loss = 1.7517597675323486, value loss = 0.33961451053619385\n",
      "INFO 2024-12-01 04:41:57 pipeline.py:712] Training Step 7740: Policy loss = 1.7038217782974243, value loss = 0.32222306728363037\n",
      "INFO 2024-12-01 04:43:01 pipeline.py:738] training_steps 7744: Validation loss: Poliy loss 1.9183893819324305, value_loss 0.40794929691025467\n",
      "INFO 2024-12-01 04:45:06 pipeline.py:712] Training Step 7760: Policy loss = 1.6849191188812256, value loss = 0.3404505252838135\n",
      "INFO 2024-12-01 04:47:42 pipeline.py:712] Training Step 7780: Policy loss = 1.6316819190979004, value loss = 0.30016547441482544\n",
      "INFO 2024-12-01 04:50:18 pipeline.py:712] Training Step 7800: Policy loss = 1.7329314947128296, value loss = 0.31335362792015076\n",
      "INFO 2024-12-01 04:52:54 pipeline.py:712] Training Step 7820: Policy loss = 1.6157327890396118, value loss = 0.30973243713378906\n",
      "INFO 2024-12-01 04:55:30 pipeline.py:712] Training Step 7840: Policy loss = 1.6599938869476318, value loss = 0.3165239691734314\n",
      "INFO 2024-12-01 04:58:06 pipeline.py:712] Training Step 7860: Policy loss = 1.7590529918670654, value loss = 0.29907965660095215\n",
      "INFO 2024-12-01 05:00:42 pipeline.py:712] Training Step 7880: Policy loss = 1.6891084909439087, value loss = 0.3052690625190735\n",
      "INFO 2024-12-01 05:03:18 pipeline.py:712] Training Step 7900: Policy loss = 1.6728157997131348, value loss = 0.298105925321579\n",
      "INFO 2024-12-01 05:05:54 pipeline.py:712] Training Step 7920: Policy loss = 1.7595947980880737, value loss = 0.30902472138404846\n",
      "INFO 2024-12-01 05:08:30 pipeline.py:712] Training Step 7940: Policy loss = 1.6641308069229126, value loss = 0.285036563873291\n",
      "INFO 2024-12-01 05:11:06 pipeline.py:712] Training Step 7960: Policy loss = 1.7349567413330078, value loss = 0.2994229197502136\n",
      "INFO 2024-12-01 05:13:42 pipeline.py:712] Training Step 7980: Policy loss = 1.73838210105896, value loss = 0.3341856002807617\n",
      "INFO 2024-12-01 05:16:18 pipeline.py:712] Training Step 8000: Policy loss = 1.6431423425674438, value loss = 0.3352360725402832\n",
      "INFO 2024-12-01 05:18:54 pipeline.py:712] Training Step 8020: Policy loss = 1.675158143043518, value loss = 0.3373693525791168\n",
      "INFO 2024-12-01 05:21:31 pipeline.py:712] Training Step 8040: Policy loss = 1.7840691804885864, value loss = 0.29825448989868164\n",
      "INFO 2024-12-01 05:24:07 pipeline.py:712] Training Step 8060: Policy loss = 1.7003309726715088, value loss = 0.2775905728340149\n",
      "INFO 2024-12-01 05:26:43 pipeline.py:712] Training Step 8080: Policy loss = 1.6804944276809692, value loss = 0.32142728567123413\n",
      "INFO 2024-12-01 05:29:19 pipeline.py:712] Training Step 8100: Policy loss = 1.7068161964416504, value loss = 0.32484152913093567\n",
      "INFO 2024-12-01 05:31:55 pipeline.py:712] Training Step 8120: Policy loss = 1.6532442569732666, value loss = 0.30055904388427734\n",
      "INFO 2024-12-01 05:34:31 pipeline.py:712] Training Step 8140: Policy loss = 1.6699405908584595, value loss = 0.3146560788154602\n",
      "INFO 2024-12-01 05:37:07 pipeline.py:712] Training Step 8160: Policy loss = 1.6366255283355713, value loss = 0.29978305101394653\n",
      "INFO 2024-12-01 05:39:43 pipeline.py:712] Training Step 8180: Policy loss = 1.7272310256958008, value loss = 0.3498515486717224\n",
      "INFO 2024-12-01 05:42:19 pipeline.py:712] Training Step 8200: Policy loss = 1.7045130729675293, value loss = 0.3126235604286194\n",
      "INFO 2024-12-01 05:44:55 pipeline.py:712] Training Step 8220: Policy loss = 1.6938440799713135, value loss = 0.2975074350833893\n",
      "INFO 2024-12-01 05:46:30 pipeline.py:738] training_steps 8228: Validation loss: Poliy loss 1.9153907523780573, value_loss 0.41137059985614216\n",
      "INFO 2024-12-01 05:48:03 pipeline.py:712] Training Step 8240: Policy loss = 1.6751278638839722, value loss = 0.27530941367149353\n",
      "INFO 2024-12-01 05:50:39 pipeline.py:712] Training Step 8260: Policy loss = 1.6716840267181396, value loss = 0.29090967774391174\n",
      "INFO 2024-12-01 05:53:15 pipeline.py:712] Training Step 8280: Policy loss = 1.6675317287445068, value loss = 0.2878177762031555\n",
      "INFO 2024-12-01 05:55:51 pipeline.py:712] Training Step 8300: Policy loss = 1.6643002033233643, value loss = 0.24939745664596558\n",
      "INFO 2024-12-01 05:58:28 pipeline.py:712] Training Step 8320: Policy loss = 1.6573905944824219, value loss = 0.3398585021495819\n",
      "INFO 2024-12-01 06:01:04 pipeline.py:712] Training Step 8340: Policy loss = 1.70832359790802, value loss = 0.30252957344055176\n",
      "INFO 2024-12-01 06:03:40 pipeline.py:712] Training Step 8360: Policy loss = 1.6638603210449219, value loss = 0.2953917980194092\n",
      "INFO 2024-12-01 06:06:16 pipeline.py:712] Training Step 8380: Policy loss = 1.648047924041748, value loss = 0.2653225064277649\n",
      "INFO 2024-12-01 06:08:52 pipeline.py:712] Training Step 8400: Policy loss = 1.7439852952957153, value loss = 0.332791268825531\n",
      "INFO 2024-12-01 06:11:28 pipeline.py:712] Training Step 8420: Policy loss = 1.7240735292434692, value loss = 0.27556103467941284\n",
      "INFO 2024-12-01 06:14:04 pipeline.py:712] Training Step 8440: Policy loss = 1.660747766494751, value loss = 0.29444441199302673\n",
      "INFO 2024-12-01 06:16:40 pipeline.py:712] Training Step 8460: Policy loss = 1.74367094039917, value loss = 0.309847354888916\n",
      "INFO 2024-12-01 06:19:16 pipeline.py:712] Training Step 8480: Policy loss = 1.607316255569458, value loss = 0.3002925515174866\n",
      "INFO 2024-12-01 06:21:52 pipeline.py:712] Training Step 8500: Policy loss = 1.642589807510376, value loss = 0.30517467856407166\n",
      "INFO 2024-12-01 06:24:28 pipeline.py:712] Training Step 8520: Policy loss = 1.7116504907608032, value loss = 0.2914886474609375\n",
      "INFO 2024-12-01 06:27:04 pipeline.py:712] Training Step 8540: Policy loss = 1.717228651046753, value loss = 0.3203181326389313\n",
      "INFO 2024-12-01 06:29:41 pipeline.py:712] Training Step 8560: Policy loss = 1.7613660097122192, value loss = 0.2964526414871216\n",
      "INFO 2024-12-01 06:32:17 pipeline.py:712] Training Step 8580: Policy loss = 1.757186770439148, value loss = 0.2784217298030853\n",
      "INFO 2024-12-01 06:34:53 pipeline.py:712] Training Step 8600: Policy loss = 1.6789884567260742, value loss = 0.3102148771286011\n",
      "INFO 2024-12-01 06:37:29 pipeline.py:712] Training Step 8620: Policy loss = 1.692907691001892, value loss = 0.29005298018455505\n",
      "INFO 2024-12-01 06:40:05 pipeline.py:712] Training Step 8640: Policy loss = 1.6919288635253906, value loss = 0.2881099581718445\n",
      "INFO 2024-12-01 06:42:41 pipeline.py:712] Training Step 8660: Policy loss = 1.7118289470672607, value loss = 0.28259533643722534\n",
      "INFO 2024-12-01 06:45:17 pipeline.py:712] Training Step 8680: Policy loss = 1.6527330875396729, value loss = 0.29715797305107117\n",
      "INFO 2024-12-01 06:47:53 pipeline.py:712] Training Step 8700: Policy loss = 1.7712041139602661, value loss = 0.28325778245925903\n",
      "INFO 2024-12-01 06:50:00 pipeline.py:738] training_steps 8712: Validation loss: Poliy loss 1.9125096739315597, value_loss 0.38724745541322425\n",
      "INFO 2024-12-01 06:51:02 pipeline.py:712] Training Step 8720: Policy loss = 1.6361666917800903, value loss = 0.31759577989578247\n",
      "INFO 2024-12-01 06:53:38 pipeline.py:712] Training Step 8740: Policy loss = 1.6285300254821777, value loss = 0.29257386922836304\n",
      "INFO 2024-12-01 06:56:14 pipeline.py:712] Training Step 8760: Policy loss = 1.608522653579712, value loss = 0.29572004079818726\n",
      "INFO 2024-12-01 06:58:50 pipeline.py:712] Training Step 8780: Policy loss = 1.6381921768188477, value loss = 0.2578960657119751\n",
      "INFO 2024-12-01 07:01:26 pipeline.py:712] Training Step 8800: Policy loss = 1.666597604751587, value loss = 0.28453201055526733\n",
      "INFO 2024-12-01 07:04:02 pipeline.py:712] Training Step 8820: Policy loss = 1.6752028465270996, value loss = 0.2858600616455078\n",
      "INFO 2024-12-01 07:06:38 pipeline.py:712] Training Step 8840: Policy loss = 1.6676883697509766, value loss = 0.26577043533325195\n",
      "INFO 2024-12-01 07:09:14 pipeline.py:712] Training Step 8860: Policy loss = 1.6008460521697998, value loss = 0.2661336064338684\n",
      "INFO 2024-12-01 07:11:50 pipeline.py:712] Training Step 8880: Policy loss = 1.697908878326416, value loss = 0.2587817907333374\n",
      "INFO 2024-12-01 07:14:26 pipeline.py:712] Training Step 8900: Policy loss = 1.7285503149032593, value loss = 0.3028389513492584\n",
      "INFO 2024-12-01 07:17:03 pipeline.py:712] Training Step 8920: Policy loss = 1.6808617115020752, value loss = 0.2440209835767746\n",
      "INFO 2024-12-01 07:19:39 pipeline.py:712] Training Step 8940: Policy loss = 1.6959419250488281, value loss = 0.32046496868133545\n",
      "INFO 2024-12-01 07:22:16 pipeline.py:712] Training Step 8960: Policy loss = 1.6149117946624756, value loss = 0.30467671155929565\n",
      "INFO 2024-12-01 07:24:52 pipeline.py:712] Training Step 8980: Policy loss = 1.7071211338043213, value loss = 0.2828899621963501\n",
      "INFO 2024-12-01 07:27:28 pipeline.py:712] Training Step 9000: Policy loss = 1.6363892555236816, value loss = 0.3188295364379883\n",
      "INFO 2024-12-01 07:30:04 pipeline.py:712] Training Step 9020: Policy loss = 1.7294375896453857, value loss = 0.2923596203327179\n",
      "INFO 2024-12-01 07:32:40 pipeline.py:712] Training Step 9040: Policy loss = 1.6658196449279785, value loss = 0.28530460596084595\n",
      "INFO 2024-12-01 07:35:16 pipeline.py:712] Training Step 9060: Policy loss = 1.7021753787994385, value loss = 0.311093807220459\n",
      "INFO 2024-12-01 07:37:52 pipeline.py:712] Training Step 9080: Policy loss = 1.7416207790374756, value loss = 0.3121466636657715\n",
      "INFO 2024-12-01 07:40:28 pipeline.py:712] Training Step 9100: Policy loss = 1.7160617113113403, value loss = 0.30708473920822144\n",
      "INFO 2024-12-01 07:43:04 pipeline.py:712] Training Step 9120: Policy loss = 1.7178823947906494, value loss = 0.2828315794467926\n",
      "INFO 2024-12-01 07:45:41 pipeline.py:712] Training Step 9140: Policy loss = 1.6947102546691895, value loss = 0.28462308645248413\n",
      "INFO 2024-12-01 07:48:17 pipeline.py:712] Training Step 9160: Policy loss = 1.678896188735962, value loss = 0.2944503426551819\n",
      "INFO 2024-12-01 07:50:53 pipeline.py:712] Training Step 9180: Policy loss = 1.6942285299301147, value loss = 0.30529308319091797\n",
      "INFO 2024-12-01 07:53:30 pipeline.py:738] training_steps 9196: Validation loss: Poliy loss 1.9244557189159706, value_loss 0.3918706364318973\n",
      "INFO 2024-12-01 07:54:01 pipeline.py:712] Training Step 9200: Policy loss = 1.638199806213379, value loss = 0.25042054057121277\n",
      "INFO 2024-12-01 07:56:37 pipeline.py:712] Training Step 9220: Policy loss = 1.5852248668670654, value loss = 0.26618826389312744\n",
      "INFO 2024-12-01 07:59:13 pipeline.py:712] Training Step 9240: Policy loss = 1.619079351425171, value loss = 0.3089112639427185\n",
      "INFO 2024-12-01 08:01:50 pipeline.py:712] Training Step 9260: Policy loss = 1.6799652576446533, value loss = 0.2924639880657196\n",
      "INFO 2024-12-01 08:04:26 pipeline.py:712] Training Step 9280: Policy loss = 1.6671468019485474, value loss = 0.3038429915904999\n",
      "INFO 2024-12-01 08:07:02 pipeline.py:712] Training Step 9300: Policy loss = 1.70005464553833, value loss = 0.2326246201992035\n",
      "INFO 2024-12-01 08:09:38 pipeline.py:712] Training Step 9320: Policy loss = 1.5760480165481567, value loss = 0.256694495677948\n",
      "INFO 2024-12-01 08:12:14 pipeline.py:712] Training Step 9340: Policy loss = 1.6818780899047852, value loss = 0.2622034549713135\n",
      "INFO 2024-12-01 08:14:50 pipeline.py:712] Training Step 9360: Policy loss = 1.6341015100479126, value loss = 0.316955029964447\n",
      "INFO 2024-12-01 08:17:27 pipeline.py:712] Training Step 9380: Policy loss = 1.6706562042236328, value loss = 0.28922489285469055\n",
      "INFO 2024-12-01 08:20:03 pipeline.py:712] Training Step 9400: Policy loss = 1.6270432472229004, value loss = 0.2625785768032074\n",
      "INFO 2024-12-01 08:22:39 pipeline.py:712] Training Step 9420: Policy loss = 1.5977351665496826, value loss = 0.280650794506073\n",
      "INFO 2024-12-01 08:25:15 pipeline.py:712] Training Step 9440: Policy loss = 1.569852352142334, value loss = 0.2656860649585724\n",
      "INFO 2024-12-01 08:27:51 pipeline.py:712] Training Step 9460: Policy loss = 1.7025680541992188, value loss = 0.2553759813308716\n",
      "INFO 2024-12-01 08:30:27 pipeline.py:712] Training Step 9480: Policy loss = 1.5766490697860718, value loss = 0.24615035951137543\n",
      "INFO 2024-12-01 08:33:04 pipeline.py:712] Training Step 9500: Policy loss = 1.6924045085906982, value loss = 0.28930577635765076\n",
      "INFO 2024-12-01 08:35:39 pipeline.py:712] Training Step 9520: Policy loss = 1.631523847579956, value loss = 0.2969275414943695\n",
      "INFO 2024-12-01 08:38:15 pipeline.py:712] Training Step 9540: Policy loss = 1.6921570301055908, value loss = 0.26441285014152527\n",
      "INFO 2024-12-01 08:40:51 pipeline.py:712] Training Step 9560: Policy loss = 1.7074618339538574, value loss = 0.3424329161643982\n",
      "INFO 2024-12-01 08:43:27 pipeline.py:712] Training Step 9580: Policy loss = 1.5848660469055176, value loss = 0.3085498809814453\n",
      "INFO 2024-12-01 08:46:03 pipeline.py:712] Training Step 9600: Policy loss = 1.6304150819778442, value loss = 0.28116118907928467\n",
      "INFO 2024-12-01 08:48:40 pipeline.py:712] Training Step 9620: Policy loss = 1.651565432548523, value loss = 0.2719918489456177\n",
      "INFO 2024-12-01 08:51:16 pipeline.py:712] Training Step 9640: Policy loss = 1.6828687191009521, value loss = 0.3297279477119446\n",
      "INFO 2024-12-01 08:53:52 pipeline.py:712] Training Step 9660: Policy loss = 1.7201855182647705, value loss = 0.2533504068851471\n",
      "INFO 2024-12-01 08:56:28 pipeline.py:712] Training Step 9680: Policy loss = 1.701967716217041, value loss = 0.2821316719055176\n",
      "INFO 2024-12-01 08:57:01 pipeline.py:738] training_steps 9680: Validation loss: Poliy loss 1.911789237475786, value_loss 0.3746683358168993\n",
      "INFO 2024-12-01 08:59:37 pipeline.py:712] Training Step 9700: Policy loss = 1.630157232284546, value loss = 0.25304290652275085\n",
      "INFO 2024-12-01 09:02:13 pipeline.py:712] Training Step 9720: Policy loss = 1.5603218078613281, value loss = 0.23917421698570251\n",
      "INFO 2024-12-01 09:04:49 pipeline.py:712] Training Step 9740: Policy loss = 1.6041905879974365, value loss = 0.24850517511367798\n",
      "INFO 2024-12-01 09:07:25 pipeline.py:712] Training Step 9760: Policy loss = 1.5862305164337158, value loss = 0.26178887486457825\n",
      "INFO 2024-12-01 09:10:01 pipeline.py:712] Training Step 9780: Policy loss = 1.5250239372253418, value loss = 0.25787678360939026\n",
      "INFO 2024-12-01 09:12:37 pipeline.py:712] Training Step 9800: Policy loss = 1.6730053424835205, value loss = 0.2548443675041199\n",
      "INFO 2024-12-01 09:15:13 pipeline.py:712] Training Step 9820: Policy loss = 1.6688705682754517, value loss = 0.245211660861969\n",
      "INFO 2024-12-01 09:17:49 pipeline.py:712] Training Step 9840: Policy loss = 1.6027806997299194, value loss = 0.2821371555328369\n",
      "INFO 2024-12-01 09:20:25 pipeline.py:712] Training Step 9860: Policy loss = 1.6464877128601074, value loss = 0.2654467821121216\n",
      "INFO 2024-12-01 09:23:02 pipeline.py:712] Training Step 9880: Policy loss = 1.7054704427719116, value loss = 0.2548115849494934\n",
      "INFO 2024-12-01 09:25:38 pipeline.py:712] Training Step 9900: Policy loss = 1.6848019361495972, value loss = 0.27091503143310547\n",
      "INFO 2024-12-01 09:28:14 pipeline.py:712] Training Step 9920: Policy loss = 1.5742290019989014, value loss = 0.2634190022945404\n",
      "INFO 2024-12-01 09:30:50 pipeline.py:712] Training Step 9940: Policy loss = 1.6539688110351562, value loss = 0.2686631679534912\n",
      "INFO 2024-12-01 09:33:26 pipeline.py:712] Training Step 9960: Policy loss = 1.621561884880066, value loss = 0.28767722845077515\n",
      "INFO 2024-12-01 09:36:02 pipeline.py:712] Training Step 9980: Policy loss = 1.6728893518447876, value loss = 0.26164063811302185\n",
      "INFO 2024-12-01 09:38:38 pipeline.py:712] Training Step 10000: Policy loss = 1.5764203071594238, value loss = 0.2852517366409302\n",
      "INFO 2024-12-01 09:41:14 pipeline.py:712] Training Step 10020: Policy loss = 1.6572966575622559, value loss = 0.22039000689983368\n",
      "INFO 2024-12-01 09:43:50 pipeline.py:712] Training Step 10040: Policy loss = 1.5290619134902954, value loss = 0.23001505434513092\n",
      "INFO 2024-12-01 09:46:26 pipeline.py:712] Training Step 10060: Policy loss = 1.6756643056869507, value loss = 0.24034181237220764\n",
      "INFO 2024-12-01 09:49:03 pipeline.py:712] Training Step 10080: Policy loss = 1.596308946609497, value loss = 0.2349008321762085\n",
      "INFO 2024-12-01 09:51:39 pipeline.py:712] Training Step 10100: Policy loss = 1.53643798828125, value loss = 0.214446559548378\n",
      "INFO 2024-12-01 09:54:15 pipeline.py:712] Training Step 10120: Policy loss = 1.627949833869934, value loss = 0.20298781991004944\n",
      "INFO 2024-12-01 09:56:51 pipeline.py:712] Training Step 10140: Policy loss = 1.5606061220169067, value loss = 0.2353244423866272\n",
      "INFO 2024-12-01 09:59:27 pipeline.py:712] Training Step 10160: Policy loss = 1.5320724248886108, value loss = 0.20619229972362518\n",
      "INFO 2024-12-01 10:00:31 pipeline.py:738] training_steps 10164: Validation loss: Poliy loss 1.868168797649321, value_loss 0.3301901563269193\n",
      "INFO 2024-12-01 10:02:36 pipeline.py:712] Training Step 10180: Policy loss = 1.4838135242462158, value loss = 0.20229686796665192\n",
      "INFO 2024-12-01 10:05:12 pipeline.py:712] Training Step 10200: Policy loss = 1.4856151342391968, value loss = 0.19770437479019165\n",
      "INFO 2024-12-01 10:07:48 pipeline.py:712] Training Step 10220: Policy loss = 1.5205882787704468, value loss = 0.19211870431900024\n",
      "INFO 2024-12-01 10:10:24 pipeline.py:712] Training Step 10240: Policy loss = 1.4611268043518066, value loss = 0.1899469792842865\n",
      "INFO 2024-12-01 10:13:00 pipeline.py:712] Training Step 10260: Policy loss = 1.4538465738296509, value loss = 0.22981402277946472\n",
      "INFO 2024-12-01 10:15:36 pipeline.py:712] Training Step 10280: Policy loss = 1.509902000427246, value loss = 0.17931905388832092\n",
      "INFO 2024-12-01 10:18:12 pipeline.py:712] Training Step 10300: Policy loss = 1.4611284732818604, value loss = 0.21178841590881348\n",
      "INFO 2024-12-01 10:20:48 pipeline.py:712] Training Step 10320: Policy loss = 1.4514644145965576, value loss = 0.196797713637352\n",
      "INFO 2024-12-01 10:23:25 pipeline.py:712] Training Step 10340: Policy loss = 1.5336878299713135, value loss = 0.19702523946762085\n",
      "INFO 2024-12-01 10:26:01 pipeline.py:712] Training Step 10360: Policy loss = 1.4504138231277466, value loss = 0.18808194994926453\n",
      "INFO 2024-12-01 10:28:37 pipeline.py:712] Training Step 10380: Policy loss = 1.4403793811798096, value loss = 0.20501311123371124\n",
      "INFO 2024-12-01 10:31:13 pipeline.py:712] Training Step 10400: Policy loss = 1.4673075675964355, value loss = 0.18625040352344513\n",
      "INFO 2024-12-01 10:33:49 pipeline.py:712] Training Step 10420: Policy loss = 1.4879846572875977, value loss = 0.19140449166297913\n",
      "INFO 2024-12-01 10:36:26 pipeline.py:712] Training Step 10440: Policy loss = 1.4112541675567627, value loss = 0.22360378503799438\n",
      "INFO 2024-12-01 10:39:02 pipeline.py:712] Training Step 10460: Policy loss = 1.4475109577178955, value loss = 0.2015410214662552\n",
      "INFO 2024-12-01 10:41:37 pipeline.py:712] Training Step 10480: Policy loss = 1.5132747888565063, value loss = 0.23326343297958374\n",
      "INFO 2024-12-01 10:44:13 pipeline.py:712] Training Step 10500: Policy loss = 1.503218412399292, value loss = 0.2010577917098999\n",
      "INFO 2024-12-01 10:46:49 pipeline.py:712] Training Step 10520: Policy loss = 1.4803458452224731, value loss = 0.2111591398715973\n",
      "INFO 2024-12-01 10:49:25 pipeline.py:712] Training Step 10540: Policy loss = 1.5038204193115234, value loss = 0.19173945486545563\n",
      "INFO 2024-12-01 10:52:01 pipeline.py:712] Training Step 10560: Policy loss = 1.5773155689239502, value loss = 0.20995910465717316\n",
      "INFO 2024-12-01 10:54:37 pipeline.py:712] Training Step 10580: Policy loss = 1.4558345079421997, value loss = 0.20470532774925232\n",
      "INFO 2024-12-01 10:57:13 pipeline.py:712] Training Step 10600: Policy loss = 1.5487974882125854, value loss = 0.20523163676261902\n",
      "INFO 2024-12-01 10:59:49 pipeline.py:712] Training Step 10620: Policy loss = 1.453444004058838, value loss = 0.21632680296897888\n",
      "INFO 2024-12-01 11:02:25 pipeline.py:712] Training Step 10640: Policy loss = 1.535921573638916, value loss = 0.22330667078495026\n",
      "INFO 2024-12-01 11:04:01 pipeline.py:738] training_steps 10648: Validation loss: Poliy loss 1.8899096143050271, value_loss 0.3201427049324161\n",
      "INFO 2024-12-01 11:05:35 pipeline.py:712] Training Step 10660: Policy loss = 1.4629592895507812, value loss = 0.18548831343650818\n",
      "INFO 2024-12-01 11:08:11 pipeline.py:712] Training Step 10680: Policy loss = 1.4856127500534058, value loss = 0.21840213239192963\n",
      "INFO 2024-12-01 11:10:46 pipeline.py:712] Training Step 10700: Policy loss = 1.4740899801254272, value loss = 0.19082996249198914\n",
      "INFO 2024-12-01 11:13:22 pipeline.py:712] Training Step 10720: Policy loss = 1.5351051092147827, value loss = 0.18626326322555542\n",
      "INFO 2024-12-01 11:15:58 pipeline.py:712] Training Step 10740: Policy loss = 1.4862643480300903, value loss = 0.1980239897966385\n",
      "INFO 2024-12-01 11:18:34 pipeline.py:712] Training Step 10760: Policy loss = 1.4636998176574707, value loss = 0.19690287113189697\n",
      "INFO 2024-12-01 11:21:10 pipeline.py:712] Training Step 10780: Policy loss = 1.4924952983856201, value loss = 0.18991997838020325\n",
      "INFO 2024-12-01 11:23:48 pipeline.py:712] Training Step 10800: Policy loss = 1.5156104564666748, value loss = 0.19293996691703796\n",
      "INFO 2024-12-01 11:26:24 pipeline.py:712] Training Step 10820: Policy loss = 1.531604528427124, value loss = 0.19746366143226624\n",
      "INFO 2024-12-01 11:29:00 pipeline.py:712] Training Step 10840: Policy loss = 1.407821536064148, value loss = 0.1953452080488205\n",
      "INFO 2024-12-01 11:31:36 pipeline.py:712] Training Step 10860: Policy loss = 1.5102721452713013, value loss = 0.2093936651945114\n",
      "INFO 2024-12-01 11:34:12 pipeline.py:712] Training Step 10880: Policy loss = 1.4961090087890625, value loss = 0.18920831382274628\n",
      "INFO 2024-12-01 11:36:48 pipeline.py:712] Training Step 10900: Policy loss = 1.4651997089385986, value loss = 0.20089799165725708\n",
      "INFO 2024-12-01 11:39:24 pipeline.py:712] Training Step 10920: Policy loss = 1.4524860382080078, value loss = 0.18941235542297363\n",
      "INFO 2024-12-01 11:42:00 pipeline.py:712] Training Step 10940: Policy loss = 1.5176842212677002, value loss = 0.18019217252731323\n",
      "INFO 2024-12-01 11:44:36 pipeline.py:712] Training Step 10960: Policy loss = 1.5021790266036987, value loss = 0.17693442106246948\n",
      "INFO 2024-12-01 11:47:12 pipeline.py:712] Training Step 10980: Policy loss = 1.4650428295135498, value loss = 0.2011585533618927\n",
      "INFO 2024-12-01 11:49:48 pipeline.py:712] Training Step 11000: Policy loss = 1.4855399131774902, value loss = 0.1797911524772644\n",
      "INFO 2024-12-01 11:52:24 pipeline.py:712] Training Step 11020: Policy loss = 1.4613189697265625, value loss = 0.1796850562095642\n",
      "INFO 2024-12-01 11:55:00 pipeline.py:712] Training Step 11040: Policy loss = 1.4972448348999023, value loss = 0.20063355565071106\n",
      "INFO 2024-12-01 11:57:36 pipeline.py:712] Training Step 11060: Policy loss = 1.4747962951660156, value loss = 0.2153552770614624\n",
      "INFO 2024-12-01 12:00:12 pipeline.py:712] Training Step 11080: Policy loss = 1.488849401473999, value loss = 0.1880818009376526\n",
      "INFO 2024-12-01 12:02:49 pipeline.py:712] Training Step 11100: Policy loss = 1.4385966062545776, value loss = 0.1873953938484192\n",
      "INFO 2024-12-01 12:05:25 pipeline.py:712] Training Step 11120: Policy loss = 1.5223987102508545, value loss = 0.17765526473522186\n",
      "INFO 2024-12-01 12:07:31 pipeline.py:738] training_steps 11132: Validation loss: Poliy loss 1.8959695345065632, value_loss 0.3194353409477922\n",
      "INFO 2024-12-01 12:08:34 pipeline.py:712] Training Step 11140: Policy loss = 1.483252763748169, value loss = 0.17173711955547333\n",
      "INFO 2024-12-01 12:11:10 pipeline.py:712] Training Step 11160: Policy loss = 1.4474315643310547, value loss = 0.19835713505744934\n",
      "INFO 2024-12-01 12:13:46 pipeline.py:712] Training Step 11180: Policy loss = 1.5159481763839722, value loss = 0.2052191197872162\n",
      "INFO 2024-12-01 12:16:22 pipeline.py:712] Training Step 11200: Policy loss = 1.5024361610412598, value loss = 0.19813108444213867\n",
      "INFO 2024-12-01 12:18:58 pipeline.py:712] Training Step 11220: Policy loss = 1.5171858072280884, value loss = 0.17533527314662933\n",
      "INFO 2024-12-01 12:21:34 pipeline.py:712] Training Step 11240: Policy loss = 1.478480339050293, value loss = 0.15549424290657043\n",
      "INFO 2024-12-01 12:24:10 pipeline.py:712] Training Step 11260: Policy loss = 1.4852046966552734, value loss = 0.1812436580657959\n",
      "INFO 2024-12-01 12:26:46 pipeline.py:712] Training Step 11280: Policy loss = 1.4035587310791016, value loss = 0.2065124213695526\n",
      "INFO 2024-12-01 12:29:22 pipeline.py:712] Training Step 11300: Policy loss = 1.449777603149414, value loss = 0.20043258368968964\n",
      "INFO 2024-12-01 12:31:58 pipeline.py:712] Training Step 11320: Policy loss = 1.5075271129608154, value loss = 0.1915205419063568\n",
      "INFO 2024-12-01 12:34:34 pipeline.py:712] Training Step 11340: Policy loss = 1.4359380006790161, value loss = 0.19341421127319336\n",
      "INFO 2024-12-01 12:37:10 pipeline.py:712] Training Step 11360: Policy loss = 1.421820878982544, value loss = 0.19086192548274994\n",
      "INFO 2024-12-01 12:39:47 pipeline.py:712] Training Step 11380: Policy loss = 1.4152079820632935, value loss = 0.1836702823638916\n",
      "INFO 2024-12-01 12:42:23 pipeline.py:712] Training Step 11400: Policy loss = 1.508998155593872, value loss = 0.1863819807767868\n",
      "INFO 2024-12-01 12:44:59 pipeline.py:712] Training Step 11420: Policy loss = 1.4254047870635986, value loss = 0.21080249547958374\n",
      "INFO 2024-12-01 12:47:35 pipeline.py:712] Training Step 11440: Policy loss = 1.4018902778625488, value loss = 0.18940292298793793\n",
      "INFO 2024-12-01 12:50:11 pipeline.py:712] Training Step 11460: Policy loss = 1.455657958984375, value loss = 0.18878977000713348\n",
      "INFO 2024-12-01 12:52:47 pipeline.py:712] Training Step 11480: Policy loss = 1.465562105178833, value loss = 0.19631034135818481\n",
      "INFO 2024-12-01 12:55:23 pipeline.py:712] Training Step 11500: Policy loss = 1.5715696811676025, value loss = 0.16800105571746826\n",
      "INFO 2024-12-01 12:57:59 pipeline.py:712] Training Step 11520: Policy loss = 1.4555935859680176, value loss = 0.1756255030632019\n",
      "INFO 2024-12-01 13:00:35 pipeline.py:712] Training Step 11540: Policy loss = 1.5337512493133545, value loss = 0.2024765908718109\n",
      "INFO 2024-12-01 13:03:11 pipeline.py:712] Training Step 11560: Policy loss = 1.470064401626587, value loss = 0.18012525141239166\n",
      "INFO 2024-12-01 13:05:47 pipeline.py:712] Training Step 11580: Policy loss = 1.4426078796386719, value loss = 0.18793708086013794\n",
      "INFO 2024-12-01 13:08:23 pipeline.py:712] Training Step 11600: Policy loss = 1.4849998950958252, value loss = 0.1863941103219986\n",
      "INFO 2024-12-01 13:11:01 pipeline.py:738] training_steps 11616: Validation loss: Poliy loss 1.899342818338363, value_loss 0.3203288332604971\n",
      "INFO 2024-12-01 13:11:33 pipeline.py:712] Training Step 11620: Policy loss = 1.449133038520813, value loss = 0.18322765827178955\n",
      "INFO 2024-12-01 13:14:09 pipeline.py:712] Training Step 11640: Policy loss = 1.4051272869110107, value loss = 0.1902765929698944\n",
      "INFO 2024-12-01 13:16:45 pipeline.py:712] Training Step 11660: Policy loss = 1.398364543914795, value loss = 0.1942196786403656\n",
      "INFO 2024-12-01 13:19:21 pipeline.py:712] Training Step 11680: Policy loss = 1.4300014972686768, value loss = 0.16692402958869934\n",
      "INFO 2024-12-01 13:21:57 pipeline.py:712] Training Step 11700: Policy loss = 1.4727360010147095, value loss = 0.16731566190719604\n",
      "INFO 2024-12-01 13:24:34 pipeline.py:712] Training Step 11720: Policy loss = 1.4482803344726562, value loss = 0.1840028017759323\n",
      "INFO 2024-12-01 13:27:09 pipeline.py:712] Training Step 11740: Policy loss = 1.3770506381988525, value loss = 0.18267369270324707\n",
      "INFO 2024-12-01 13:29:46 pipeline.py:712] Training Step 11760: Policy loss = 1.4190342426300049, value loss = 0.17426195740699768\n",
      "INFO 2024-12-01 13:32:22 pipeline.py:712] Training Step 11780: Policy loss = 1.4986555576324463, value loss = 0.16516649723052979\n",
      "INFO 2024-12-01 13:34:58 pipeline.py:712] Training Step 11800: Policy loss = 1.493243932723999, value loss = 0.19378432631492615\n",
      "INFO 2024-12-01 13:37:34 pipeline.py:712] Training Step 11820: Policy loss = 1.5328762531280518, value loss = 0.19610533118247986\n",
      "INFO 2024-12-01 13:40:10 pipeline.py:712] Training Step 11840: Policy loss = 1.4634181261062622, value loss = 0.19744348526000977\n",
      "INFO 2024-12-01 13:42:46 pipeline.py:712] Training Step 11860: Policy loss = 1.4203553199768066, value loss = 0.16472765803337097\n",
      "INFO 2024-12-01 13:45:22 pipeline.py:712] Training Step 11880: Policy loss = 1.4258252382278442, value loss = 0.20414891839027405\n",
      "INFO 2024-12-01 13:47:58 pipeline.py:712] Training Step 11900: Policy loss = 1.4434998035430908, value loss = 0.16920757293701172\n",
      "INFO 2024-12-01 13:50:34 pipeline.py:712] Training Step 11920: Policy loss = 1.428636074066162, value loss = 0.16954055428504944\n",
      "INFO 2024-12-01 13:53:10 pipeline.py:712] Training Step 11940: Policy loss = 1.3714065551757812, value loss = 0.20039939880371094\n",
      "INFO 2024-12-01 13:55:47 pipeline.py:712] Training Step 11960: Policy loss = 1.4250739812850952, value loss = 0.17572668194770813\n",
      "INFO 2024-12-01 13:58:23 pipeline.py:712] Training Step 11980: Policy loss = 1.4382859468460083, value loss = 0.16021360456943512\n",
      "INFO 2024-12-01 14:00:59 pipeline.py:712] Training Step 12000: Policy loss = 1.4449626207351685, value loss = 0.16567003726959229\n",
      "INFO 2024-12-01 14:03:35 pipeline.py:712] Training Step 12020: Policy loss = 1.5457031726837158, value loss = 0.1834779530763626\n",
      "INFO 2024-12-01 14:06:11 pipeline.py:712] Training Step 12040: Policy loss = 1.473724603652954, value loss = 0.16919752955436707\n",
      "INFO 2024-12-01 14:08:47 pipeline.py:712] Training Step 12060: Policy loss = 1.5451633930206299, value loss = 0.20656129717826843\n",
      "INFO 2024-12-01 14:11:23 pipeline.py:712] Training Step 12080: Policy loss = 1.4532570838928223, value loss = 0.18371941149234772\n",
      "INFO 2024-12-01 14:13:59 pipeline.py:712] Training Step 12100: Policy loss = 1.4768502712249756, value loss = 0.1847299039363861\n",
      "INFO 2024-12-01 14:14:32 pipeline.py:738] training_steps 12100: Validation loss: Poliy loss 1.9027940826337846, value_loss 0.3229244461313623\n",
      "INFO 2024-12-01 14:17:08 pipeline.py:712] Training Step 12120: Policy loss = 1.472791314125061, value loss = 0.1703822910785675\n",
      "INFO 2024-12-01 14:19:44 pipeline.py:712] Training Step 12140: Policy loss = 1.4497562646865845, value loss = 0.19849970936775208\n",
      "INFO 2024-12-01 14:22:20 pipeline.py:712] Training Step 12160: Policy loss = 1.482574462890625, value loss = 0.16418588161468506\n",
      "INFO 2024-12-01 14:24:57 pipeline.py:712] Training Step 12180: Policy loss = 1.5551950931549072, value loss = 0.1664772629737854\n",
      "INFO 2024-12-01 14:27:33 pipeline.py:712] Training Step 12200: Policy loss = 1.3665869235992432, value loss = 0.15939316153526306\n",
      "INFO 2024-12-01 14:30:09 pipeline.py:712] Training Step 12220: Policy loss = 1.3942002058029175, value loss = 0.17046044766902924\n",
      "INFO 2024-12-01 14:32:45 pipeline.py:712] Training Step 12240: Policy loss = 1.4596583843231201, value loss = 0.1899503767490387\n",
      "INFO 2024-12-01 14:35:21 pipeline.py:712] Training Step 12260: Policy loss = 1.4825503826141357, value loss = 0.1565978080034256\n",
      "INFO 2024-12-01 14:37:57 pipeline.py:712] Training Step 12280: Policy loss = 1.424699306488037, value loss = 0.16500744223594666\n",
      "INFO 2024-12-01 14:40:33 pipeline.py:712] Training Step 12300: Policy loss = 1.482866644859314, value loss = 0.17711392045021057\n",
      "INFO 2024-12-01 14:43:09 pipeline.py:712] Training Step 12320: Policy loss = 1.4940071105957031, value loss = 0.18667766451835632\n",
      "INFO 2024-12-01 14:45:46 pipeline.py:712] Training Step 12340: Policy loss = 1.4266760349273682, value loss = 0.18716749548912048\n",
      "INFO 2024-12-01 14:48:21 pipeline.py:712] Training Step 12360: Policy loss = 1.4536123275756836, value loss = 0.16264642775058746\n",
      "INFO 2024-12-01 14:50:57 pipeline.py:712] Training Step 12380: Policy loss = 1.435802936553955, value loss = 0.19306059181690216\n",
      "INFO 2024-12-01 14:53:33 pipeline.py:712] Training Step 12400: Policy loss = 1.4587774276733398, value loss = 0.18620458245277405\n",
      "INFO 2024-12-01 14:56:09 pipeline.py:712] Training Step 12420: Policy loss = 1.4789657592773438, value loss = 0.15628919005393982\n",
      "INFO 2024-12-01 14:58:45 pipeline.py:712] Training Step 12440: Policy loss = 1.4606943130493164, value loss = 0.205190971493721\n",
      "INFO 2024-12-01 15:01:21 pipeline.py:712] Training Step 12460: Policy loss = 1.463417649269104, value loss = 0.23961246013641357\n",
      "INFO 2024-12-01 15:03:57 pipeline.py:712] Training Step 12480: Policy loss = 1.4387704133987427, value loss = 0.19158947467803955\n",
      "INFO 2024-12-01 15:06:33 pipeline.py:712] Training Step 12500: Policy loss = 1.4206557273864746, value loss = 0.2018669843673706\n",
      "INFO 2024-12-01 15:09:09 pipeline.py:712] Training Step 12520: Policy loss = 1.447721242904663, value loss = 0.17506349086761475\n",
      "INFO 2024-12-01 15:11:45 pipeline.py:712] Training Step 12540: Policy loss = 1.4778037071228027, value loss = 0.19095589220523834\n",
      "INFO 2024-12-01 15:14:22 pipeline.py:712] Training Step 12560: Policy loss = 1.402519941329956, value loss = 0.20145101845264435\n",
      "INFO 2024-12-01 15:16:58 pipeline.py:712] Training Step 12580: Policy loss = 1.4088647365570068, value loss = 0.18632405996322632\n",
      "INFO 2024-12-01 15:18:02 pipeline.py:738] training_steps 12584: Validation loss: Poliy loss 1.9081573222504287, value_loss 0.3219700056265612\n",
      "INFO 2024-12-01 15:20:07 pipeline.py:712] Training Step 12600: Policy loss = 1.4067857265472412, value loss = 0.17773312330245972\n",
      "INFO 2024-12-01 15:22:43 pipeline.py:712] Training Step 12620: Policy loss = 1.3907355070114136, value loss = 0.1772056221961975\n",
      "INFO 2024-12-01 15:25:19 pipeline.py:712] Training Step 12640: Policy loss = 1.4346811771392822, value loss = 0.17146015167236328\n",
      "INFO 2024-12-01 15:27:55 pipeline.py:712] Training Step 12660: Policy loss = 1.4188835620880127, value loss = 0.15919551253318787\n",
      "INFO 2024-12-01 15:30:32 pipeline.py:712] Training Step 12680: Policy loss = 1.4895552396774292, value loss = 0.17114469408988953\n",
      "INFO 2024-12-01 15:33:08 pipeline.py:712] Training Step 12700: Policy loss = 1.4237232208251953, value loss = 0.19067621231079102\n",
      "INFO 2024-12-01 15:35:44 pipeline.py:712] Training Step 12720: Policy loss = 1.4612698554992676, value loss = 0.18074999749660492\n",
      "INFO 2024-12-01 15:38:20 pipeline.py:712] Training Step 12740: Policy loss = 1.448150873184204, value loss = 0.15145918726921082\n",
      "INFO 2024-12-01 15:40:56 pipeline.py:712] Training Step 12760: Policy loss = 1.365164041519165, value loss = 0.1900966763496399\n",
      "INFO 2024-12-01 15:43:32 pipeline.py:712] Training Step 12780: Policy loss = 1.5007957220077515, value loss = 0.1852564811706543\n",
      "INFO 2024-12-01 15:46:08 pipeline.py:712] Training Step 12800: Policy loss = 1.4745228290557861, value loss = 0.19113397598266602\n",
      "INFO 2024-12-01 15:48:44 pipeline.py:712] Training Step 12820: Policy loss = 1.3851008415222168, value loss = 0.16052907705307007\n",
      "INFO 2024-12-01 15:51:20 pipeline.py:712] Training Step 12840: Policy loss = 1.4035570621490479, value loss = 0.18178322911262512\n",
      "INFO 2024-12-01 15:53:56 pipeline.py:712] Training Step 12860: Policy loss = 1.4410786628723145, value loss = 0.1862974762916565\n",
      "INFO 2024-12-01 15:56:32 pipeline.py:712] Training Step 12880: Policy loss = 1.4845727682113647, value loss = 0.1706022024154663\n",
      "INFO 2024-12-01 15:59:09 pipeline.py:712] Training Step 12900: Policy loss = 1.4483098983764648, value loss = 0.1697615683078766\n",
      "INFO 2024-12-01 16:01:44 pipeline.py:712] Training Step 12920: Policy loss = 1.4353090524673462, value loss = 0.17135313153266907\n",
      "INFO 2024-12-01 16:04:20 pipeline.py:712] Training Step 12940: Policy loss = 1.456182599067688, value loss = 0.19180116057395935\n",
      "INFO 2024-12-01 16:06:56 pipeline.py:712] Training Step 12960: Policy loss = 1.5216925144195557, value loss = 0.18385738134384155\n",
      "INFO 2024-12-01 16:09:32 pipeline.py:712] Training Step 12980: Policy loss = 1.41428804397583, value loss = 0.18630149960517883\n",
      "INFO 2024-12-01 16:12:08 pipeline.py:712] Training Step 13000: Policy loss = 1.416285753250122, value loss = 0.17207637429237366\n",
      "INFO 2024-12-01 16:14:44 pipeline.py:712] Training Step 13020: Policy loss = 1.4411206245422363, value loss = 0.18709971010684967\n",
      "INFO 2024-12-01 16:17:20 pipeline.py:712] Training Step 13040: Policy loss = 1.5268112421035767, value loss = 0.1830548346042633\n",
      "INFO 2024-12-01 16:19:56 pipeline.py:712] Training Step 13060: Policy loss = 1.4827234745025635, value loss = 0.17343641817569733\n",
      "INFO 2024-12-01 16:21:32 pipeline.py:738] training_steps 13068: Validation loss: Poliy loss 1.9138836313466556, value_loss 0.32397401442781826\n",
      "INFO 2024-12-01 16:23:06 pipeline.py:712] Training Step 13080: Policy loss = 1.4155901670455933, value loss = 0.19935482740402222\n",
      "INFO 2024-12-01 16:25:42 pipeline.py:712] Training Step 13100: Policy loss = 1.4254672527313232, value loss = 0.17128022015094757\n",
      "INFO 2024-12-01 16:28:18 pipeline.py:712] Training Step 13120: Policy loss = 1.3185378313064575, value loss = 0.20694774389266968\n",
      "INFO 2024-12-01 16:30:54 pipeline.py:712] Training Step 13140: Policy loss = 1.4643800258636475, value loss = 0.16772066056728363\n",
      "INFO 2024-12-01 16:33:30 pipeline.py:712] Training Step 13160: Policy loss = 1.464667797088623, value loss = 0.15699502825737\n",
      "INFO 2024-12-01 16:36:06 pipeline.py:712] Training Step 13180: Policy loss = 1.4753432273864746, value loss = 0.15679648518562317\n",
      "INFO 2024-12-01 16:41:09 pipeline.py:712] Training Step 13200: Policy loss = 1.4547463655471802, value loss = 0.16120587289333344\n",
      "INFO 2024-12-01 16:46:23 pipeline.py:712] Training Step 13220: Policy loss = 1.4291311502456665, value loss = 0.1862674057483673\n",
      "INFO 2024-12-01 16:51:37 pipeline.py:712] Training Step 13240: Policy loss = 1.396214246749878, value loss = 0.1677359640598297\n",
      "INFO 2024-12-01 16:56:52 pipeline.py:712] Training Step 13260: Policy loss = 1.4267948865890503, value loss = 0.20656993985176086\n",
      "INFO 2024-12-01 17:02:06 pipeline.py:712] Training Step 13280: Policy loss = 1.46268892288208, value loss = 0.18324315547943115\n",
      "INFO 2024-12-01 17:07:20 pipeline.py:712] Training Step 13300: Policy loss = 1.4505820274353027, value loss = 0.20377781987190247\n",
      "INFO 2024-12-01 17:12:34 pipeline.py:712] Training Step 13320: Policy loss = 1.37784743309021, value loss = 0.20381806790828705\n",
      "INFO 2024-12-01 17:17:48 pipeline.py:712] Training Step 13340: Policy loss = 1.4602909088134766, value loss = 0.17335352301597595\n",
      "INFO 2024-12-01 17:23:02 pipeline.py:712] Training Step 13360: Policy loss = 1.3968043327331543, value loss = 0.15956124663352966\n",
      "INFO 2024-12-01 17:28:17 pipeline.py:712] Training Step 13380: Policy loss = 1.4018046855926514, value loss = 0.1630624532699585\n",
      "INFO 2024-12-01 17:33:32 pipeline.py:712] Training Step 13400: Policy loss = 1.431191086769104, value loss = 0.18240591883659363\n",
      "INFO 2024-12-01 17:38:47 pipeline.py:712] Training Step 13420: Policy loss = 1.4688524007797241, value loss = 0.18472030758857727\n",
      "INFO 2024-12-01 17:44:02 pipeline.py:712] Training Step 13440: Policy loss = 1.3972108364105225, value loss = 0.1343834102153778\n",
      "INFO 2024-12-01 17:49:18 pipeline.py:712] Training Step 13460: Policy loss = 1.4538371562957764, value loss = 0.15936720371246338\n",
      "INFO 2024-12-01 17:54:33 pipeline.py:712] Training Step 13480: Policy loss = 1.465029239654541, value loss = 0.17111258208751678\n",
      "INFO 2024-12-01 17:59:50 pipeline.py:712] Training Step 13500: Policy loss = 1.4598513841629028, value loss = 0.19228507578372955\n",
      "INFO 2024-12-01 18:05:08 pipeline.py:712] Training Step 13520: Policy loss = 1.43364679813385, value loss = 0.18019263446331024\n",
      "INFO 2024-12-02 10:40:35 pipeline.py:712] Training Step 13540: Policy loss = 1.4097013473510742, value loss = 0.17679202556610107\n",
      "INFO 2024-12-02 10:42:42 pipeline.py:738] training_steps 13552: Validation loss: Poliy loss 1.9209579629976241, value_loss 0.32471374141388254\n",
      "INFO 2024-12-02 10:43:44 pipeline.py:712] Training Step 13560: Policy loss = 1.451925277709961, value loss = 0.1734681874513626\n",
      "INFO 2024-12-02 10:46:18 pipeline.py:712] Training Step 13580: Policy loss = 1.4625022411346436, value loss = 0.17687740921974182\n",
      "INFO 2024-12-02 10:48:52 pipeline.py:712] Training Step 13600: Policy loss = 1.3880256414413452, value loss = 0.18846789002418518\n",
      "INFO 2024-12-02 10:51:28 pipeline.py:712] Training Step 13620: Policy loss = 1.4215978384017944, value loss = 0.1764083206653595\n",
      "INFO 2024-12-02 10:54:02 pipeline.py:712] Training Step 13640: Policy loss = 1.4012686014175415, value loss = 0.19972530007362366\n",
      "INFO 2024-12-02 10:56:36 pipeline.py:712] Training Step 13660: Policy loss = 1.4476675987243652, value loss = 0.19027982652187347\n",
      "INFO 2024-12-02 10:59:10 pipeline.py:712] Training Step 13680: Policy loss = 1.3500235080718994, value loss = 0.17733509838581085\n",
      "INFO 2024-12-02 11:01:45 pipeline.py:712] Training Step 13700: Policy loss = 1.4869208335876465, value loss = 0.17798860371112823\n",
      "INFO 2024-12-02 11:04:19 pipeline.py:712] Training Step 13720: Policy loss = 1.4228827953338623, value loss = 0.2006654292345047\n",
      "INFO 2024-12-02 11:06:53 pipeline.py:712] Training Step 13740: Policy loss = 1.4214956760406494, value loss = 0.17019736766815186\n",
      "INFO 2024-12-02 11:09:27 pipeline.py:712] Training Step 13760: Policy loss = 1.4130463600158691, value loss = 0.16907013952732086\n",
      "INFO 2024-12-02 11:12:01 pipeline.py:712] Training Step 13780: Policy loss = 1.3581231832504272, value loss = 0.18849247694015503\n",
      "INFO 2024-12-02 11:14:36 pipeline.py:712] Training Step 13800: Policy loss = 1.38005530834198, value loss = 0.2091713547706604\n",
      "INFO 2024-12-02 11:17:10 pipeline.py:712] Training Step 13820: Policy loss = 1.489347219467163, value loss = 0.17760717868804932\n",
      "INFO 2024-12-02 11:19:45 pipeline.py:712] Training Step 13840: Policy loss = 1.4323045015335083, value loss = 0.17164599895477295\n",
      "INFO 2024-12-02 11:22:19 pipeline.py:712] Training Step 13860: Policy loss = 1.4230762720108032, value loss = 0.17772971093654633\n",
      "INFO 2024-12-02 11:24:53 pipeline.py:712] Training Step 13880: Policy loss = 1.4912447929382324, value loss = 0.18496742844581604\n",
      "INFO 2024-12-02 11:27:27 pipeline.py:712] Training Step 13900: Policy loss = 1.3986151218414307, value loss = 0.16866230964660645\n",
      "INFO 2024-12-02 11:30:01 pipeline.py:712] Training Step 13920: Policy loss = 1.4750251770019531, value loss = 0.16838258504867554\n",
      "INFO 2024-12-02 11:32:36 pipeline.py:712] Training Step 13940: Policy loss = 1.3281594514846802, value loss = 0.16894462704658508\n",
      "INFO 2024-12-02 11:35:11 pipeline.py:712] Training Step 13960: Policy loss = 1.48909592628479, value loss = 0.15711385011672974\n",
      "INFO 2024-12-02 11:37:46 pipeline.py:712] Training Step 13980: Policy loss = 1.4741318225860596, value loss = 0.17547568678855896\n",
      "INFO 2024-12-02 11:40:20 pipeline.py:712] Training Step 14000: Policy loss = 1.4592199325561523, value loss = 0.18807251751422882\n",
      "INFO 2024-12-02 11:42:54 pipeline.py:712] Training Step 14020: Policy loss = 1.5438461303710938, value loss = 0.190199613571167\n",
      "INFO 2024-12-02 11:45:30 pipeline.py:738] training_steps 14036: Validation loss: Poliy loss 1.9233279316151728, value_loss 0.32696076641317273\n",
      "INFO 2024-12-02 11:46:01 pipeline.py:712] Training Step 14040: Policy loss = 1.3917694091796875, value loss = 0.16401778161525726\n",
      "INFO 2024-12-02 11:48:36 pipeline.py:712] Training Step 14060: Policy loss = 1.3970649242401123, value loss = 0.1709377020597458\n",
      "INFO 2024-12-02 11:51:10 pipeline.py:712] Training Step 14080: Policy loss = 1.355154037475586, value loss = 0.17702855169773102\n",
      "INFO 2024-12-02 11:53:44 pipeline.py:712] Training Step 14100: Policy loss = 1.420593023300171, value loss = 0.16143691539764404\n",
      "INFO 2024-12-02 11:56:18 pipeline.py:712] Training Step 14120: Policy loss = 1.4654226303100586, value loss = 0.1661967933177948\n",
      "INFO 2024-12-02 11:58:52 pipeline.py:712] Training Step 14140: Policy loss = 1.4706337451934814, value loss = 0.19554466009140015\n",
      "INFO 2024-12-02 12:01:26 pipeline.py:712] Training Step 14160: Policy loss = 1.3621264696121216, value loss = 0.19794943928718567\n",
      "INFO 2024-12-02 12:04:00 pipeline.py:712] Training Step 14180: Policy loss = 1.3670823574066162, value loss = 0.1821240931749344\n",
      "INFO 2024-12-02 12:06:35 pipeline.py:712] Training Step 14200: Policy loss = 1.380721092224121, value loss = 0.18678325414657593\n",
      "INFO 2024-12-02 12:09:09 pipeline.py:712] Training Step 14220: Policy loss = 1.4142577648162842, value loss = 0.16519570350646973\n",
      "INFO 2024-12-02 12:11:43 pipeline.py:712] Training Step 14240: Policy loss = 1.4471113681793213, value loss = 0.17861023545265198\n",
      "INFO 2024-12-02 12:14:18 pipeline.py:712] Training Step 14260: Policy loss = 1.4187995195388794, value loss = 0.16397294402122498\n",
      "INFO 2024-12-02 12:16:52 pipeline.py:712] Training Step 14280: Policy loss = 1.428499460220337, value loss = 0.20211008191108704\n",
      "INFO 2024-12-02 12:19:26 pipeline.py:712] Training Step 14300: Policy loss = 1.5255275964736938, value loss = 0.16947825253009796\n",
      "INFO 2024-12-02 12:22:00 pipeline.py:712] Training Step 14320: Policy loss = 1.3467469215393066, value loss = 0.1674354076385498\n",
      "INFO 2024-12-02 12:24:34 pipeline.py:712] Training Step 14340: Policy loss = 1.4382762908935547, value loss = 0.17080023884773254\n",
      "INFO 2024-12-02 12:27:08 pipeline.py:712] Training Step 14360: Policy loss = 1.4359118938446045, value loss = 0.1946171522140503\n",
      "INFO 2024-12-02 12:29:43 pipeline.py:712] Training Step 14380: Policy loss = 1.4725956916809082, value loss = 0.1797156184911728\n",
      "INFO 2024-12-02 12:32:17 pipeline.py:712] Training Step 14400: Policy loss = 1.4192607402801514, value loss = 0.17017534375190735\n",
      "INFO 2024-12-02 12:34:55 pipeline.py:712] Training Step 14420: Policy loss = 1.5189323425292969, value loss = 0.17766213417053223\n",
      "INFO 2024-12-02 12:37:32 pipeline.py:712] Training Step 14440: Policy loss = 1.4738383293151855, value loss = 0.19402819871902466\n",
      "INFO 2024-12-02 12:40:11 pipeline.py:712] Training Step 14460: Policy loss = 1.4667654037475586, value loss = 0.16889634728431702\n",
      "INFO 2024-12-02 12:42:50 pipeline.py:712] Training Step 14480: Policy loss = 1.5163700580596924, value loss = 0.19082510471343994\n",
      "INFO 2024-12-02 12:45:28 pipeline.py:712] Training Step 14500: Policy loss = 1.4380069971084595, value loss = 0.1655016839504242\n",
      "INFO 2024-12-02 12:48:05 pipeline.py:712] Training Step 14520: Policy loss = 1.4450385570526123, value loss = 0.15912902355194092\n",
      "INFO 2024-12-02 12:48:39 pipeline.py:738] training_steps 14520: Validation loss: Poliy loss 1.9288236635630247, value_loss 0.32841214537620544\n",
      "INFO 2024-12-02 12:51:17 pipeline.py:712] Training Step 14540: Policy loss = 1.4051133394241333, value loss = 0.19770655035972595\n",
      "INFO 2024-12-02 12:53:56 pipeline.py:712] Training Step 14560: Policy loss = 1.415871024131775, value loss = 0.1730818748474121\n",
      "INFO 2024-12-02 12:56:30 pipeline.py:712] Training Step 14580: Policy loss = 1.447693109512329, value loss = 0.19282621145248413\n",
      "INFO 2024-12-02 12:59:03 pipeline.py:712] Training Step 14600: Policy loss = 1.4170550107955933, value loss = 0.1770526021718979\n",
      "INFO 2024-12-02 13:01:36 pipeline.py:712] Training Step 14620: Policy loss = 1.377218246459961, value loss = 0.1858363151550293\n",
      "INFO 2024-12-02 13:04:10 pipeline.py:712] Training Step 14640: Policy loss = 1.4279444217681885, value loss = 0.1963401734828949\n",
      "INFO 2024-12-02 13:06:43 pipeline.py:712] Training Step 14660: Policy loss = 1.5398719310760498, value loss = 0.18719476461410522\n",
      "INFO 2024-12-02 13:09:16 pipeline.py:712] Training Step 14680: Policy loss = 1.4067381620407104, value loss = 0.15639996528625488\n",
      "INFO 2024-12-02 13:11:49 pipeline.py:712] Training Step 14700: Policy loss = 1.3601889610290527, value loss = 0.17445310950279236\n",
      "INFO 2024-12-02 13:14:23 pipeline.py:712] Training Step 14720: Policy loss = 1.4268429279327393, value loss = 0.18212394416332245\n",
      "INFO 2024-12-02 13:16:56 pipeline.py:712] Training Step 14740: Policy loss = 1.3477668762207031, value loss = 0.15259304642677307\n",
      "INFO 2024-12-02 13:19:29 pipeline.py:712] Training Step 14760: Policy loss = 1.3855109214782715, value loss = 0.18045377731323242\n",
      "INFO 2024-12-02 13:22:03 pipeline.py:712] Training Step 14780: Policy loss = 1.4994972944259644, value loss = 0.1804606020450592\n",
      "INFO 2024-12-02 13:24:36 pipeline.py:712] Training Step 14800: Policy loss = 1.4533103704452515, value loss = 0.16714327037334442\n",
      "INFO 2024-12-02 13:27:09 pipeline.py:712] Training Step 14820: Policy loss = 1.3472161293029785, value loss = 0.17722836136817932\n",
      "INFO 2024-12-02 13:29:43 pipeline.py:712] Training Step 14840: Policy loss = 1.462350606918335, value loss = 0.19481541216373444\n",
      "INFO 2024-12-02 13:32:16 pipeline.py:712] Training Step 14860: Policy loss = 1.428120732307434, value loss = 0.1576678454875946\n",
      "INFO 2024-12-02 13:34:48 pipeline.py:712] Training Step 14880: Policy loss = 1.4435373544692993, value loss = 0.15890926122665405\n",
      "INFO 2024-12-02 13:37:21 pipeline.py:712] Training Step 14900: Policy loss = 1.4458177089691162, value loss = 0.17302493751049042\n",
      "INFO 2024-12-02 13:39:54 pipeline.py:712] Training Step 14920: Policy loss = 1.4986151456832886, value loss = 0.17423458397388458\n",
      "INFO 2024-12-02 13:42:27 pipeline.py:712] Training Step 14940: Policy loss = 1.4461320638656616, value loss = 0.19170942902565002\n",
      "INFO 2024-12-02 13:45:00 pipeline.py:712] Training Step 14960: Policy loss = 1.4502248764038086, value loss = 0.20655032992362976\n",
      "INFO 2024-12-02 13:47:33 pipeline.py:712] Training Step 14980: Policy loss = 1.437147855758667, value loss = 0.16933734714984894\n",
      "INFO 2024-12-02 13:50:06 pipeline.py:712] Training Step 15000: Policy loss = 1.4635775089263916, value loss = 0.16441752016544342\n",
      "INFO 2024-12-02 13:51:09 pipeline.py:738] training_steps 15004: Validation loss: Poliy loss 1.933623067668227, value_loss 0.33049958504614285\n",
      "INFO 2024-12-02 13:53:11 pipeline.py:712] Training Step 15020: Policy loss = 1.3554002046585083, value loss = 0.17079420387744904\n",
      "INFO 2024-12-02 13:55:44 pipeline.py:712] Training Step 15040: Policy loss = 1.3356770277023315, value loss = 0.19468778371810913\n",
      "INFO 2024-12-02 13:58:16 pipeline.py:712] Training Step 15060: Policy loss = 1.375382423400879, value loss = 0.1805402636528015\n",
      "INFO 2024-12-02 14:00:49 pipeline.py:712] Training Step 15080: Policy loss = 1.437400460243225, value loss = 0.1800737977027893\n",
      "INFO 2024-12-02 14:03:22 pipeline.py:712] Training Step 15100: Policy loss = 1.5006639957427979, value loss = 0.1760045886039734\n",
      "INFO 2024-12-02 14:05:55 pipeline.py:712] Training Step 15120: Policy loss = 1.347760796546936, value loss = 0.1889723241329193\n",
      "INFO 2024-12-02 14:08:28 pipeline.py:712] Training Step 15140: Policy loss = 1.4061861038208008, value loss = 0.18944455683231354\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     14\u001b[0m     learner_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43msupervised_learner_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearner_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43margument_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margument_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_training_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_training_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neural-search/alpha_zero/core/pipeline.py:697\u001b[0m, in \u001b[0;36msupervised_learner_loop\u001b[0;34m(seed, network, data_dir, optimizer, lr_scheduler, device, logger, argument_data, batch_size, ckpt_interval, log_interval, max_training_steps, patience, ckpt_dir, logs_dir)\u001b[0m\n\u001b[1;32m    695\u001b[0m pi_loss, v_loss \u001b[38;5;241m=\u001b[39m compute_supervised_losses(network, device, transition, argument_data)\n\u001b[1;32m    696\u001b[0m loss \u001b[38;5;241m=\u001b[39m pi_loss \u001b[38;5;241m+\u001b[39m v_loss\n\u001b[0;32m--> 697\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    699\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/neural-search/venv/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neural-search/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neural-search/venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "set_seed(FLAGS.seed)\n",
    "\n",
    "maybe_create_dir(FLAGS.ckpt_dir)\n",
    "maybe_create_dir(FLAGS.logs_dir)\n",
    "# maybe_create_dir(FLAGS.save_sgf_dir)\n",
    "\n",
    "logger = create_logger(FLAGS.log_level)\n",
    "\n",
    "logger.info(extract_args_from_flags_dict(FLAGS.flag_values_dict()))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    learner_device = torch.device('cuda')\n",
    "supervised_learner_loop(\n",
    "    seed = FLAGS.seed,\n",
    "    network = network,\n",
    "    data_dir = FLAGS.dataset_dir,\n",
    "    device = learner_device,\n",
    "    optimizer = optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    logger = logger,\n",
    "    argument_data = FLAGS.argument_data,\n",
    "    batch_size = FLAGS.batch_size,\n",
    "    ckpt_interval = FLAGS.ckpt_interval,\n",
    "    log_interval = FLAGS.log_interval,\n",
    "    max_training_steps = FLAGS.max_training_steps,\n",
    "    patience = 1000,\n",
    "    ckpt_dir = FLAGS.ckpt_dir,\n",
    "    logs_dir = FLAGS.logs_dir,\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hook_fn(module, input, output):\n",
    "#     print(f\"Input shape: {module}, {input[0].shape}\")  # input is a tuple; get the shape of the first element\n",
    "#     print(f\"Output shape:{module}, {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the hook on the first layer of conv_block1\n",
    "# hook_handle = network.conv_block.register_forward_hook(hook_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
