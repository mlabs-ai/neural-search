{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains the AlphaZero agent on a single machine for the game of Go.\"\"\"\n",
    "import os\n",
    "\n",
    "# This forces OpenMP to use 1 single thread, which is needed to\n",
    "# prevent contention between multiple process.\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# Tell numpy to only use one core.\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "import sys\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('board_size', 9, 'Board size for Go.')\n",
    "flags.DEFINE_float('komi', 7.5, 'Komi rule for Go.')\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'num_stack',\n",
    "    8,\n",
    "    'Stack N previous states, the state is an image of N x 2 + 1 binary planes.',\n",
    ")\n",
    "\n",
    "flags.DEFINE_integer('num_filters', 236, 'Number of filters for the conv2d layers in the neural network.')\n",
    "flags.DEFINE_integer('max_depth', 10, ' maximum depth for quantum search')\n",
    "flags.DEFINE_integer('branching_width', 3, ' branching_width for quantum search')\n",
    "flags.DEFINE_integer('beam_width', 1, ' beam_width for quantum search')\n",
    "flags.DEFINE_integer(\n",
    "    'num_fc_units',\n",
    "    128,\n",
    "    'Number of hidden units in the linear layer of the neural network.',\n",
    ")\n",
    "flags.DEFINE_integer('num_search', 1, ' number of search modules for quantum search')\n",
    "\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'batch_size',\n",
    "    1024,\n",
    "    'To avoid overfitting, we want to make sure the agent only sees ~10% of samples in the replay over one checkpoint.'\n",
    "    'That is, batch_size * ckpt_interval <= replay_capacity * 0.1',\n",
    ")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    'argument_data',\n",
    "    True,\n",
    "    'Apply random rotation and mirroring to the training data, default on.',\n",
    ")\n",
    "\n",
    "\n",
    "flags.DEFINE_float('init_lr', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_float('lr_decay', 0.1, 'Learning rate decay rate.')\n",
    "flags.DEFINE_multi_integer(\n",
    "    'lr_milestones',\n",
    "    [10000, 20000, 40000],\n",
    "    'The number of training steps at which the learning rate will be decayed.',\n",
    ")\n",
    "flags.DEFINE_float('l2_regularization', 1e-4, 'The L2 regularization parameter applied to weights.')\n",
    "flags.DEFINE_float('sgd_momentum', 0.9, '')\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'max_training_steps',\n",
    "    int(5e4),\n",
    "    'Number of training steps (measured in network parameter update, one batch is one training step).',\n",
    ")\n",
    "\n",
    "flags.DEFINE_integer('ckpt_interval', 500, 'The frequency (in training step) to create new checkpoint.')\n",
    "flags.DEFINE_integer('log_interval', 20, 'The frequency (in training step) to log training statistics.')\n",
    "\n",
    "flags.DEFINE_string('ckpt_dir', '', 'Checkpoint directory (to be generated dynamically)')\n",
    "flags.DEFINE_string('logs_dir', '', 'Logs directory (to be generated dynamically)')\n",
    "flags.DEFINE_string(\n",
    "    'dataset_dir',\n",
    "    'go_dataset.pth',\n",
    "    'Go dataset',\n",
    ")\n",
    "\n",
    "flags.DEFINE_string('log_level', 'INFO', '')\n",
    "flags.DEFINE_integer('seed', 1, 'Seed the runtime.')\n",
    "\n",
    "\n",
    "# Initialize flags\n",
    "FLAGS(sys.argv, known_only = True)\n",
    "\n",
    "os.environ['BOARD_SIZE'] = str(FLAGS.board_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_folder_name(depth, search, branching, filters, beam):\n",
    "    return f\"d_{depth}s_{search}br_{branching}f_{filters}be_{beam}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = generate_folder_name(\n",
    "        FLAGS.max_depth, FLAGS.num_search, FLAGS.branching_width, FLAGS.num_filters, FLAGS.beam_width\n",
    "    )\n",
    "\n",
    "# Update ckpt_dir and logs_dir with the generated folder name\n",
    "FLAGS.ckpt_dir = f'./checkpoints/go/9x9/quantum/{folder_name}'\n",
    "FLAGS.logs_dir = f'./logs/go/9x9/quantum/{folder_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_zero.envs.go import GoEnv\n",
    "from alpha_zero.core.pipeline import (\n",
    "    supervised_learner_loop,\n",
    "    set_seed,\n",
    "    maybe_create_dir,\n",
    ")\n",
    "from alpha_zero.core.quantum_net import QuantumAlphaZeroNet\n",
    "from alpha_zero.utils.util import extract_args_from_flags_dict, create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_builder():\n",
    "        return GoEnv(komi=FLAGS.komi, num_stack=FLAGS.num_stack)\n",
    "eval_env = env_builder()\n",
    "\n",
    "input_shape = eval_env.observation_space.shape\n",
    "num_actions = eval_env.action_space.n\n",
    "def network_builder():\n",
    "        return QuantumAlphaZeroNet(\n",
    "            input_shape,\n",
    "            num_actions,\n",
    "            FLAGS.num_filters,\n",
    "            FLAGS.max_depth,\n",
    "            FLAGS.branching_width,\n",
    "            FLAGS.beam_width,\n",
    "            FLAGS.num_fc_units,\n",
    "            FLAGS.num_search\n",
    "\n",
    "        )\n",
    "network = network_builder()\n",
    "network = torch.compile(network)\n",
    "optimizer = torch.optim.SGD(\n",
    "    network.parameters(),\n",
    "    lr=FLAGS.init_lr,\n",
    "    momentum=FLAGS.sgd_momentum,\n",
    "    weight_decay=FLAGS.l2_regularization,\n",
    ")\n",
    "lr_scheduler = MultiStepLR(optimizer, milestones=FLAGS.lr_milestones, gamma=FLAGS.lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in network.parameters())\n",
    "print(f\" Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "set_seed(FLAGS.seed)\n",
    "\n",
    "maybe_create_dir(FLAGS.ckpt_dir)\n",
    "maybe_create_dir(FLAGS.logs_dir)\n",
    "# maybe_create_dir(FLAGS.save_sgf_dir)\n",
    "\n",
    "logger = create_logger(FLAGS.log_level)\n",
    "\n",
    "logger.info(extract_args_from_flags_dict(FLAGS.flag_values_dict()))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    learner_device = torch.device('cuda')\n",
    "supervised_learner_loop(\n",
    "    seed = FLAGS.seed,\n",
    "    network = network,\n",
    "    data_dir = FLAGS.dataset_dir,\n",
    "    device = learner_device,\n",
    "    optimizer = optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    logger = logger,\n",
    "    argument_data = FLAGS.argument_data,\n",
    "    batch_size = FLAGS.batch_size,\n",
    "    ckpt_interval = FLAGS.ckpt_interval,\n",
    "    log_interval = FLAGS.log_interval,\n",
    "    max_training_steps = FLAGS.max_training_steps,\n",
    "    patience = 1000,\n",
    "    ckpt_dir = FLAGS.ckpt_dir,\n",
    "    logs_dir = FLAGS.logs_dir,\n",
    "   )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
