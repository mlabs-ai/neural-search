{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# This forces OpenMP to use 1 single thread, which is needed to\n",
    "# prevent contention between multiple process.\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "# Tell numpy to only use one core.\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "from absl import flags\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('board_size', 9, 'Board size for Go.')\n",
    "flags.DEFINE_float('komi', 7.5, 'Komi rule for Go.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_stack',\n",
    "    8,\n",
    "    'Stack N previous states, the state is an image of N x 2 + 1 binary planes.',\n",
    ")\n",
    "flags.DEFINE_integer('num_res_blocks', 10, 'Number of residual blocks in the neural network.')\n",
    "flags.DEFINE_integer('num_filters_resnet', 128, 'Number of filters for the conv2d layers in the neural network.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_fc_units',\n",
    "    128,\n",
    "    'Number of hidden units in the linear layer of the neural network.',\n",
    ")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'num_simulations',\n",
    "    200,\n",
    "    'Number of simulations per MCTS search, this applies to both self-play and evaluation processes.',\n",
    ")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    'num_parallel',\n",
    "    8,\n",
    "    'Number of leaves to collect before using the neural network to evaluate the positions during MCTS search,'\n",
    "    '1 means no parallel search.',\n",
    ")\n",
    "flags.DEFINE_float(\n",
    "    'c_puct_base',\n",
    "    19652,\n",
    "    'Exploration constants balancing priors vs. search values. Original paper use 19652',\n",
    ")\n",
    "flags.DEFINE_float(\n",
    "    'c_puct_init',\n",
    "    1.25,\n",
    "    'Exploration constants balancing priors vs. search values. Original paper use 1.25',\n",
    ")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    'default_rating',\n",
    "    1500,\n",
    "    'Default elo rating, change to the rating (for black) from last checkpoint when resume training.',\n",
    ")\n",
    "flags.DEFINE_string(\n",
    "    'logs_dir',\n",
    "    './logs/go/9x9/alphago_series',\n",
    "    'Path to save statistics for self-play, training, and evaluation.',\n",
    ")\n",
    "flags.DEFINE_string('log_level', 'INFO', '')\n",
    "flags.DEFINE_integer('seed', 1, 'Seed the runtime.')\n",
    "# Initialize flags\n",
    "FLAGS(sys.argv, known_only = True)\n",
    "\n",
    "os.environ['BOARD_SIZE'] = str(FLAGS.board_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plink failed to import tkinter.\n"
     ]
    }
   ],
   "source": [
    "from alpha_zero.envs.go import GoEnv\n",
    "from alpha_zero.core.pipeline import (\n",
    "    set_seed,\n",
    "    maybe_create_dir,\n",
    ")\n",
    "from alpha_zero.core.multi_game import run_tournament\n",
    "from alpha_zero.core.quantum_net import QuantumAlphaZeroNet\n",
    "from alpha_zero.core.network import AlphaZeroNet\n",
    "from alpha_zero.utils.util import extract_args_from_flags_dict, create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_configs = [\n",
    "    {\n",
    "        'name': 'Agent_1',\n",
    "        'num_filters': 128,\n",
    "        'max_depth': 3,\n",
    "        'branching_width': 3,\n",
    "        'beam_width': 1,\n",
    "        'num_fc_units':128,\n",
    "        'num_search':3,\n",
    "        'load_chkpt' : './checkpoints/go/9x9/quantum/d_3s_3br_3f_128be_1/training_steps_11500.ckpt'\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'name': 'Agent_2',\n",
    "        'num_filters': 128,\n",
    "        'max_depth': 5,\n",
    "        'branching_width': 3,\n",
    "        'beam_width': 1,\n",
    "        'num_fc_units':128,\n",
    "        'num_search':2,\n",
    "        'load_chkpt' : './checkpoints/go/9x9/quantum/d_5s_2br_3f_128be_1/training_steps_10500.ckpt'\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'name': 'Agent_3',\n",
    "        'num_filters': 84,\n",
    "        'max_depth': 2,\n",
    "        'branching_width': 3,\n",
    "        'beam_width': 1,\n",
    "        'num_fc_units':128,\n",
    "        'num_search':5,\n",
    "        'load_chkpt' : './checkpoints/go/9x9/quantum/d_2s_5br_3f_84be_1/training_steps_11500.ckpt'\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_builder():\n",
    "        return GoEnv(komi=FLAGS.komi, num_stack=FLAGS.num_stack)\n",
    "eval_env = env_builder()\n",
    "\n",
    "input_shape = eval_env.observation_space.shape\n",
    "num_actions = eval_env.action_space.n\n",
    "\n",
    "# Initialize agents\n",
    "Agents = []\n",
    "for config in agent_configs:\n",
    "    agent = QuantumAlphaZeroNet(\n",
    "        input_shape,\n",
    "        num_actions,\n",
    "        config['num_filters'],\n",
    "        config['max_depth'],\n",
    "        config['branching_width'],\n",
    "        config['beam_width'],\n",
    "        config['num_fc_units'],\n",
    "        config['num_search'],\n",
    "    )\n",
    "    Agents.append(agent)\n",
    "\n",
    "resnet_agent = AlphaZeroNet(\n",
    "            input_shape,\n",
    "            num_actions,\n",
    "            FLAGS.num_res_blocks,\n",
    "            FLAGS.num_filters_resnet,\n",
    "            FLAGS.num_fc_units,\n",
    "        )\n",
    "resnet_chkpt = './checkpoints/go/9x9/resnets/network/training_steps_5500.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents with metadata\n",
    "agents = {\n",
    "    \"ResNet\": {\n",
    "        \"network\": resnet_agent,\n",
    "        \"elo_rating\": 1200,  # Initial Elo rating\n",
    "        \"checkpoint\": resnet_chkpt,\n",
    "        \"wins\": 0,\n",
    "        \"lost\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add QuantumAlphaZeroNet agents\n",
    "for config, agent in zip(agent_configs, Agents):\n",
    "    agents[config[\"name\"]] = {\n",
    "        \"network\": agent,\n",
    "        \"elo_rating\": 1200,  # Initial Elo rating\n",
    "        \"checkpoint\": config[\"load_chkpt\"],\n",
    "        \"wins\": 0,\n",
    "        \"lost\":0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def visualize_all_elo_ratings(agents: dict, logs_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a bar chart of Elo ratings for all agents.\n",
    "\n",
    "    Args:\n",
    "        agents (dict): Dictionary containing agent information, including Elo ratings.\n",
    "        logs_dir (str): Directory to save the Elo ratings chart.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Extract agent names and Elo ratings\n",
    "    agent_names = list(agents.keys())\n",
    "    elo_ratings = [agent[\"elo_rating\"] for agent in agents.values()]\n",
    "\n",
    "    # Create the bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(agent_names, elo_ratings, color=\"skyblue\")\n",
    "    plt.xlabel(\"Agents\")\n",
    "    plt.ylabel(\"Elo Rating\")\n",
    "    plt.title(\"Elo Ratings for All Agents\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate labels for better readability\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and display the plot\n",
    "    output_path = os.path.join(logs_dir, \"elo_ratings_all_agents.png\")\n",
    "    os.makedirs(logs_dir, exist_ok=True)\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Elo ratings chart saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2024-12-02 17:37:51 3364720299.py:5] {'board_size': 9, 'komi': 7.5, 'num_stack': 8, 'num_res_blocks': 10, 'num_filters_resnet': 128, 'num_fc_units': 128, 'num_simulations': 200, 'num_parallel': 8, 'c_puct_base': 19652.0, 'c_puct_init': 1.25, 'default_rating': 1500.0, 'logs_dir': './logs/go/9x9/alphago_series', 'log_level': 'INFO', 'seed': 1}\n"
     ]
    }
   ],
   "source": [
    "set_seed(FLAGS.seed)\n",
    "\n",
    "logger = create_logger(FLAGS.log_level)\n",
    "\n",
    "logger.info(extract_args_from_flags_dict(FLAGS.flag_values_dict()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2024-12-02 17:37:51 multi_game.py:324] Starting match between ResNet and Agent_1\n",
      "/home/banashree/neural-search/alpha_zero/core/multi_game.py:335: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_state_1 = torch.load(agents[agent_1_name][\"checkpoint\"], map_location=device)\n",
      "INFO 2024-12-02 17:37:52 multi_game.py:339] network 1 model loaded from checkpoint: ./checkpoints/go/9x9/resnets/network/training_steps_5500.ckpt\n",
      "/home/banashree/neural-search/alpha_zero/core/multi_game.py:342: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_state_2 = torch.load(agents[agent_2_name][\"checkpoint\"], map_location=device)\n",
      "INFO 2024-12-02 17:37:52 multi_game.py:345] network 2 model loaded from checkpoint: ./checkpoints/go/9x9/quantum/d_3s_3br_3f_128be_1/training_steps_11500.ckpt\n",
      "INFO 2024-12-02 17:37:52 multi_game.py:370] Playing game 1/10 between ResNet and Agent_1\n",
      "INFO 2024-12-02 17:37:52 multi_game.py:374] Game 1: ResNet is Black, Agent_1 is White\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      2\u001b[0m     learner_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrun_tournament\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43magents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearner_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_simulations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_parallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_puct_base\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_puct_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_puct_init\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_puct_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_rating\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_rating\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neural-search/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neural-search/alpha_zero/core/multi_game.py:403\u001b[0m, in \u001b[0;36mrun_tournament\u001b[0;34m(seed, agents, env, device, num_games, num_simulations, num_parallel, c_puct_base, c_puct_init, default_rating, log_level, logs_dir)\u001b[0m\n\u001b[1;32m    401\u001b[0m         white_wins \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    402\u001b[0m         agents[current_white_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwins\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 403\u001b[0m         agents[current_black_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlost\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# Update Elo ratings after all games\u001b[39;00m\n\u001b[1;32m    407\u001b[0m agents[current_black_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melo_rating\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_black_elo\u001b[38;5;241m.\u001b[39mrating\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lost'"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    learner_device = torch.device('cuda')\n",
    "\n",
    "run_tournament(\n",
    "    seed = FLAGS.seed,\n",
    "    agents = agents,\n",
    "    env = eval_env,\n",
    "    device = learner_device,\n",
    "    num_games = 10,\n",
    "    num_simulations = FLAGS.num_simulations,\n",
    "    num_parallel = FLAGS.num_parallel,\n",
    "    c_puct_base = FLAGS.c_puct_base,\n",
    "    c_puct_init = FLAGS.c_puct_init,\n",
    "    default_rating = FLAGS.default_rating,\n",
    "    log_level = FLAGS.log_level,\n",
    "    logs_dir = FLAGS.logs_dir,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hook_fn(module, input, output):\n",
    "#     print(f\"Input shape: {module}, {input[0].shape}\")  # input is a tuple; get the shape of the first element\n",
    "#     print(f\"Output shape:{module}, {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the hook on the first layer of conv_block1\n",
    "# hook_handle = network.conv_block.register_forward_hook(hook_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
